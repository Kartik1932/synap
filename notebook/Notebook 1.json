{
	"name": "Notebook 1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkSuper",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "sparkConfiguration2",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "9ee831d0-1697-41df-a4d3-58690037cbec"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/c3c7b119-f070-4a96-9c51-d78e1e9ddafc/resourceGroups/kartikRG/providers/Microsoft.Synapse/workspaces/synapsekartik/bigDataPools/SparkSuper",
				"name": "SparkSuper",
				"type": "Spark",
				"endpoint": "https://synapsekartik.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkSuper",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "sparkConfiguration2"
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import logging\n",
					"import codecs\n",
					"import sys\n",
					"import datetime\n",
					"from datetime import *\n",
					"import re\n",
					"import os\n",
					"import pyspark\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql import *\n",
					"try:\n",
					"    import pandas as pd\n",
					"    import numpy as np\n",
					"except:\n",
					"    logging.info(\"error importing pandas/numpy\")\n",
					"from datetime import datetime, timedelta\n",
					"import json\n",
					"import heapq\n",
					"import tempfile\n",
					"\n",
					"# Sources\n",
					"\n",
					"WEBSITE      = 'website'\n",
					"PRESENTER    = 'presenter'\n",
					"GOOGLE_DRIVE = 'GoogleDrive'\n",
					"BOX          = 'Box'\n",
					"SHARE_POINT  = 'SharePoint'\n",
					"GMAIL        = 'Gmail'\n",
					"EXCHANGE     = 'Exchange'\n",
					"OUTLOOK_PLUGIN = 'OutlookPlugin'\n",
					"\n",
					"SOURCES_STR_MAP = {\n",
					"        WEBSITE: 'Docurated',\n",
					"        PRESENTER: 'Presenter',\n",
					"        GOOGLE_DRIVE: 'Google Drive',\n",
					"        BOX: 'Box',\n",
					"        SHARE_POINT: 'SharePoint',\n",
					"        GMAIL: 'Gmail',\n",
					"        EXCHANGE: 'Exchange',\n",
					"        OUTLOOK_PLUGIN: 'OutlookPlugin'\n",
					"        }\n",
					"\n",
					"# Activity Type\n",
					"\n",
					"VIEW        = 'view'\n",
					"PREZHIDDEN  = 'prezhidden'\n",
					"VIEW_END     = 'viewend'\n",
					"OPEN        = 'open'\n",
					"DOWNLOAD    = 'download'\n",
					"MODIFY      = 'modify'\n",
					"CLIP        = 'clip'\n",
					"SHARE       = 'share'\n",
					"GRANT_DOCUMENT_ACCESS       = 'manageaccess'\n",
					"COPY_LINK   = 'copylink'\n",
					"EXPORT      = 'export'\n",
					"\n",
					"ANALYTICS_VIEW = 'analyticsview'\n",
					"PRESENTER_ANALYTICS_VIEW = 'presenter_analyticsview'\n",
					"\n",
					"WIDGET_VIEW = 'widgetview'\n",
					"\n",
					"SEARCH      = 'search'\n",
					"TAG         = 'tag'\n",
					"OPPORTUNITY = 'opportunity'\n",
					"\n",
					"TOPIC       = 'topic'\n",
					"\n",
					"LOGIN       = 'login'\n",
					"\n",
					"CREATE      = 'create'\n",
					"\n",
					"COLLECTION  = 'collection'\n",
					"\n",
					"COMPONENT_START = 'component_start'\n",
					"\n",
					"COMPONENT_END = 'component_end'\n",
					"\n",
					"COMPONENT_START_INTERNAL = 'internal_component_start'\n",
					"\n",
					"COMPONENT_END_INTERNAL = 'internal_component_end'\n",
					"\n",
					"TYPES = [\n",
					"        VIEW, VIEW_END, OPEN, DOWNLOAD, MODIFY, CLIP, SHARE, EXPORT, SEARCH, TAG, COMPONENT_START, COMPONENT_END, PREZHIDDEN,\n",
					"        OPPORTUNITY, TOPIC, COLLECTION, LOGIN, ANALYTICS_VIEW, WIDGET_VIEW, CREATE, GRANT_DOCUMENT_ACCESS, COPY_LINK, PRESENTER_ANALYTICS_VIEW,\n",
					"        COMPONENT_START_INTERNAL, COMPONENT_END_INTERNAL\n",
					"        ]\n",
					"\n",
					"TYPES_STR_MAP = {\n",
					"    VIEW: \"Views\",\n",
					"    VIEW_END: \"View End\",\n",
					"    PREZHIDDEN: \"Prezhidden\",\n",
					"    OPEN: \"Opens\",\n",
					"    DOWNLOAD: \"Downloads\",\n",
					"    MODIFY: \"Modifies\",\n",
					"    CLIP: \"Clips\",\n",
					"    SHARE: \"Shares\",\n",
					"    COPY_LINK: \"Copy Link\",\n",
					"    GRANT_DOCUMENT_ACCESS: \"Manage Document Access\",\n",
					"    EXPORT: \"Exports\",\n",
					"    SEARCH: \"Searches\",\n",
					"    TAG: \"Favorites/Comments\",\n",
					"    OPPORTUNITY: \"Opportunity Actions\",\n",
					"    COMPONENT_START: \"Component Actions\",\n",
					"    COMPONENT_END: \"Component Actions\",\n",
					"    TOPIC: \"Topic Actions\",\n",
					"    COLLECTION: \"Collection Actions\",\n",
					"    LOGIN: \"Logins\",\n",
					"    ANALYTICS_VIEW: \"Analytics Views\",\n",
					"    PRESENTER_ANALYTICS_VIEW: \"Presenter Analytics Views\",\n",
					"    WIDGET_VIEW: \"Widget Tab Views\",\n",
					"    CREATE: \"Create\",\n",
					"    COMPONENT_START_INTERNAL: \"internal start component\",\n",
					"    COMPONENT_END_INTERNAL: \"internal end component\"\n",
					"}\n",
					"\n",
					"\n",
					"# Mappings\n",
					"\n",
					"EVENT_TYPE_MAP = {}\n",
					"EVENT_TYPE_MAP[WEBSITE] = {\n",
					"    \"space_show\":                                    VIEW,\n",
					"    \"space_inline_show\":                             VIEW,\n",
					"    \"space_render_space_buttons\":                    VIEW,\n",
					"    \"tiles_show\":                                    VIEW,\n",
					"\n",
					"    \"space_close\":                                   VIEW_END,\n",
					"\n",
					"    \"docurated_profile_analytics\":                   PRESENTER_ANALYTICS_VIEW,\n",
					"    \"admin_usage_dashboard\":                         ANALYTICS_VIEW,\n",
					"    \"admin_content_activity\":                        ANALYTICS_VIEW,\n",
					"    \"admin_user_searches\":                           ANALYTICS_VIEW,\n",
					"    \"admin_topic_actions\":                           ANALYTICS_VIEW,\n",
					"    \"admin_top_performing_files\":                    ANALYTICS_VIEW,\n",
					"    \"admin_content_usage_in_deals\":                  ANALYTICS_VIEW,\n",
					"    \"admin_content_usage_by_rep\":                    ANALYTICS_VIEW,\n",
					"    \"admin_processing\":                              ANALYTICS_VIEW,\n",
					"    \"plus/api/tableau_view_url\":                     ANALYTICS_VIEW,\n",
					"\n",
					"    \"plus/api/api_opp_widget_view-search\":           WIDGET_VIEW,\n",
					"    \"plus/api/api_opp_widget_view-attachments\":      WIDGET_VIEW,\n",
					"    \"plus/api/api_opp_widget_view-suggestions\":      WIDGET_VIEW,\n",
					"    \"plus/api/api_opp_widget_view-timeline\":         WIDGET_VIEW,\n",
					"\n",
					"    \"authorized_share_space\":                        GRANT_DOCUMENT_ACCESS,\n",
					"    \"plus/api/spaces_share_space\":                   GRANT_DOCUMENT_ACCESS,\n",
					"    \"space_share_clipboard\":                         GRANT_DOCUMENT_ACCESS,\n",
					"    \"plus/api/spaces_share_clipboard\":               GRANT_DOCUMENT_ACCESS,\n",
					"    \"presentation_create_send-email\":                SHARE,\n",
					"    \"presentation_create_create-link\":               SHARE,\n",
					"    \"plus/api/presentation_create_create-link\":      SHARE,\n",
					"    \"plus/api/presentation_create_send-email\":       SHARE,\n",
					"    \"presentation_sharing_link_copied\":              COPY_LINK,\n",
					"\n",
					"    \"space_open_by_space_id\":                        OPEN,\n",
					"    \"space_linked_space_opener_by_space\":            OPEN,\n",
					"    \"space_open_by_tile_id\":                         OPEN,\n",
					"\n",
					"    \"docurated_update_clipboard\":                    CLIP,\n",
					"    \"docurated_clip_space\":                          CLIP,\n",
					"    \"plus/api/clipboard_clip_space\":                 CLIP,\n",
					"    \"docurated_clip_tile\":                           CLIP,\n",
					"    \"plus/api/clipboard_clip_tile\":                  CLIP,\n",
					"    \"space_save_clipboard\":                          CLIP,\n",
					"    \"plus/api/spaces_save_clipboard\":                CLIP,\n",
					"    \"tiles_extracted_images\":                        CLIP,\n",
					"\n",
					"    \"media_download\":                                DOWNLOAD,\n",
					"    \"media_export\":                                  EXPORT,\n",
					"    \"plus/api/clipboard_export\":                     EXPORT,\n",
					"\n",
					"    \"plus/api/search_index\":                         SEARCH,\n",
					"    \"search_index\":                                  SEARCH,\n",
					"    \"search_list_view\":                              SEARCH,\n",
					"    \"search_document_search\":                        SEARCH,\n",
					"\n",
					"    \"comments_create\":                               TAG,\n",
					"    \"favorites_create\":                              TAG,\n",
					"    \"plus/api/favorites_add_to_favorite\":            TAG,\n",
					"\n",
					"    \"sfdc/canvas/clipboard_attach_to_object\":        OPPORTUNITY,\n",
					"    \"sfdc/canvas/attachment_attach_to_object\":       OPPORTUNITY,\n",
					"    \"sfdc/canvas/attachment_attach_to_opportunity\":  OPPORTUNITY,\n",
					"\n",
					"    \"topics_create\":                                 TOPIC,\n",
					"    \"topics_edit\":                                   TOPIC,\n",
					"    \"topics_destroy\":                                TOPIC,\n",
					"    \"topics_add_space\":                              TOPIC,\n",
					"    \"topics_remove_space\":                           TOPIC,\n",
					"    \"topics_add_folder_topic\":                       TOPIC,\n",
					"    \"topics_remove_folder_topic\":                    TOPIC,\n",
					"    \"topics_index\":                                  TOPIC,\n",
					"    \"plus/api/topics_index\":                         TOPIC,\n",
					"    \"topics_share\":                                  TOPIC,\n",
					"    \"topics_show\":                                   TOPIC,\n",
					"\n",
					"    \"collections_create\":                            COLLECTION,\n",
					"    \"collections_duplicate\":                         COLLECTION,\n",
					"    \"collections_edit\":                              COLLECTION,\n",
					"    \"collections_destroy\":                           COLLECTION,\n",
					"    \"plus/api/collections_delete\":                   COLLECTION,\n",
					"    \"collections_add_space\":                         COLLECTION,\n",
					"    \"collections_remove_space\":                      COLLECTION,\n",
					"    \"collections_index\":                             COLLECTION,\n",
					"    \"plus/api/collections_index\":                    COLLECTION,\n",
					"    \"collections_share\":                             COLLECTION,\n",
					"    \"plus/api/collections_authorize_users\":          COLLECTION,\n",
					"    \"collections_show\":                              COLLECTION,\n",
					"\n",
					"    \"auth/sessions_create\":                          LOGIN,\n",
					"\n",
					"    \"space_create\":                                  CREATE,\n",
					"\n",
					"    \"internal_component_start\":                      COMPONENT_START_INTERNAL,\n",
					"    \"internal_component_end\":                        COMPONENT_END_INTERNAL\n",
					"}\n",
					"\n",
					"EVENT_TYPE_MAP[PRESENTER] = {\n",
					"    \"presentation_remote_updates_user-joins\":       VIEW,\n",
					"    \"presentation_remote_updates_remote-closed\":    VIEW_END,\n",
					"    \"presentation_remote_updates_remote-hidden\":    PREZHIDDEN,\n",
					"    \"presentation_remote_updates_remote-download\":  DOWNLOAD,\n",
					"    \"presentation_remote_updates_user-joins-collection\":        COLLECTION,\n",
					"    \"presentation_remote_updates_remote-collection-download\":   COLLECTION,\n",
					"    \"presentation_remote_updates_component-start\":      COMPONENT_START,\n",
					"    \"presentation_remote_updates_component-end\":      COMPONENT_END\n",
					"}\n",
					"\n",
					"EVENT_TYPE_MAP[GMAIL] = {\n",
					"    \"email\":  SHARE\n",
					"}\n",
					"\n",
					"EVENT_TYPE_MAP[GOOGLE_DRIVE] = {\n",
					"    \"view\":    VIEW,\n",
					"    \"modify\":  MODIFY\n",
					"}\n",
					"\n",
					"EVENT_TYPE_MAP[BOX] = {\n",
					"    \"view\":      VIEW,\n",
					"    \"preview\":   VIEW,\n",
					"    \"download\":  DOWNLOAD,\n",
					"    \"modify\":    MODIFY\n",
					"}\n",
					"\n",
					"EVENT_TYPE_MAP[EXCHANGE] = {\n",
					"    \"email\":  SHARE\n",
					"}\n",
					"\n",
					"EVENT_TYPE_MAP[OUTLOOK_PLUGIN] = {\n",
					"    \"email\":  SHARE\n",
					"}\n",
					"\n",
					"# No sharepoint events at the moment.\n",
					"EVENT_TYPE_MAP[SHARE_POINT] = {\n",
					"   \"view\":      VIEW,\n",
					"   \"preview\":   VIEW,\n",
					"   \"download\":  DOWNLOAD,\n",
					"   \"modify\":    MODIFY\n",
					"}\n",
					"\n",
					"VALUES_STR_MAP = {\n",
					"    \"space_show\":                                    \"View a document\",\n",
					"    \"space_inline_show\":                             \"View a document inline\",\n",
					"    \"space_render_space_buttons\":                    \"View a document\",\n",
					"    \"tiles_show\":                                    \"View a page\",\n",
					"\n",
					"    \"space_close\":                                   \"View closed for a document\",\n",
					"\n",
					"    \"internal_component_start\":                      \"Component start internal\",\n",
					"    \"internal_component_end\":                        \"Component end internal\",\n",
					"\n",
					"    \"docurated_profile_analytics\":                   \"View Presenter Analytics\",\n",
					"    \"admin_usage_dashboard\":                         \"View Site Usage Analytics\",\n",
					"    \"admin_content_activity\":                        \"View Content Activity Analytics\",\n",
					"    \"admin_user_searches\":                           \"View User Searches Analytics\",\n",
					"    \"admin_topic_actions\":                           \"View Topic Actions Analytics\",\n",
					"    \"admin_top_performing_files\":                    \"View Top Performing Files Analytics\",\n",
					"    \"admin_content_usage_in_deals\":                  \"View Content Usage in Deals Analytics\",\n",
					"    \"admin_content_usage_by_rep\":                    \"View Content Usage by Rep Analytics\",\n",
					"    \"admin_processing\":                              \"View Processing Analytics\",\n",
					"    \"plus/api/tableau_view_url\":                     \"View Tableau Analytics\",\n",
					"\n",
					"    \"plus/api/api_opp_widget_view-search\":           \"Search from widget\",\n",
					"    \"plus/api/api_opp_widget_view-attachments\":      \"View attachments in widget\",\n",
					"    \"plus/api/api_opp_widget_view-suggestions\":      \"View suggestions in widget\",\n",
					"    \"plus/api/api_opp_widget_view-timeline\":         \"View timeline in widget\",\n",
					"\n",
					"    \"authorized_share_space\":                        \"Share a document\",\n",
					"    \"plus/api/spaces_share_space\":                   \"Share a document\",\n",
					"    \"space_share_clipboard\":                         \"Share clipboard contents\",\n",
					"    \"plus/api/spaces_share_clipboard\":               \"Share clipboard contents\",\n",
					"    \"presentation_create_send-email\":                \"Share a presenter link\",\n",
					"    \"plus/api/presentation_create_send-email\":       \"Share a presenter link\",\n",
					"    \"presentation_create_create-link\":               \"Share a presenter link\",\n",
					"    \"plus/api/presentation_create_create-link\":      \"Share a presenter link\",\n",
					"    \"presentation_sharing_link_copied\":              \"Copy a presenter link\",\n",
					"\n",
					"    \"space_open_by_space_id\":                        \"Open a document\",\n",
					"    \"space_linked_space_opener_by_space\":            \"Open a document\",\n",
					"    \"space_open_by_tile_id\":                         \"Open a document\",\n",
					"\n",
					"    \"docurated_update_clipboard\":                    \"Clip a document\",\n",
					"    \"docurated_clip_space\":                          \"Clip a document\",\n",
					"    \"plus/api/clipboard_clip_space\":                 \"Clip a document\",\n",
					"    \"docurated_clip_tile\":                           \"Clip a page\",\n",
					"    \"plus/api/clipboard_clip_tile\":                  \"Clip a page\",\n",
					"    \"space_save_clipboard\":                          \"Save clipboard content\",\n",
					"    \"plus/api/spaces_save_clipboard\":                \"Save clipboard content\",\n",
					"    \"tiles_extracted_images\":                        \"Clip objects from page\",\n",
					"\n",
					"    \"media_download\":                                \"Download a document\",\n",
					"    \"media_export\":                                  \"Export a document\",\n",
					"    \"plus/api/clipboard_export\":                     \"Export a document\",\n",
					"    \"plus/api/search_index\":                         \"Perform search\",\n",
					"    \"search_index\":                                  \"Perform search\",\n",
					"    \"search_list_view\":                              \"Perform search\",\n",
					"    \"search_document_search\":                        \"Perform search\",\n",
					"\n",
					"    \"comments_create\":                               \"Create comment\",\n",
					"    \"favorites_create\":                              \"Favorite a document\",\n",
					"    \"plus/api/favorites_add_to_favorite\":            \"Favorite a document\",\n",
					"\n",
					"    \"sfdc/canvas/clipboard_attach_to_object\":        \"Attach document to opportunity\",\n",
					"    \"sfdc/canvas/attachment_attach_to_object\":       \"Attach document to opportunity\",\n",
					"    \"sfdc/canvas/attachment_attach_to_opportunity\":  \"Attach document to opportunity\",\n",
					"\n",
					"    \"topics_create\":                                 \"Create topic\",\n",
					"    \"topics_edit\":                                   \"Edit topic\",\n",
					"    \"topics_destroy\":                                \"Delete topic\",\n",
					"    \"topics_add_space\":                              \"Add document to topic\",\n",
					"    \"topics_remove_space\":                           \"Remove document from topic\",\n",
					"    \"topics_add_folder_topic\":                       \"Add folder mapping to topic\",\n",
					"    \"topics_remove_folder_topic\":                    \"Remove folder mapping from topic\",\n",
					"    \"topics_index\":                                  \"View topic\",\n",
					"    \"plus/api/topics_index\":                         \"View topic\",\n",
					"    \"topics_share\":                                  \"Share topic\",\n",
					"    \"topics_show\":                                   \"View topic\",\n",
					"\n",
					"    \"collections_create\":                             \"Create\",\n",
					"    \"collections_duplicate\":                          \"Duplicate\",\n",
					"    \"collections_edit\":                               \"Edit\",\n",
					"    \"collections_destroy\":                            \"Delete\",\n",
					"    \"plus/api/collections_delete\":                    \"Delete\",\n",
					"    \"collections_add_space\":                          \"Add document\",\n",
					"    \"collections_remove_space\":                       \"Remove document\",\n",
					"    \"collections_index\":                              \"View\",\n",
					"    \"plus/api/collections_index\":                     \"View\",\n",
					"    \"collections_share\":                              \"Share\",\n",
					"    \"plus/api/collections_authorize_users\":           \"Share\",\n",
					"    \"collections_show\":                               \"View\",\n",
					"\n",
					"    \"presentation_remote_updates_user-joins\":                   \"User joins presentation\",\n",
					"    \"presentation_remote_updates_user-joins-collection\":        \"Presenter View\",\n",
					"    \"presentation_remote_updates_remote-unhidden\":              \"User re-focuses presentation\",\n",
					"    \"presentation_remote_updates_remote-hidden\":                \"User hide the presentation\",\n",
					"    \"presentation_remote_updates_remote-download\":              \"User downloads presentation\",\n",
					"    \"presentation_remote_updates_remote-collection-download\":   \"Presenter Download\",\n",
					"    \"presentation_remote_updates_component-start\":              \"Component Start\",\n",
					"    \"presentation_remote_updates_component-end\":              \"Component End\",\n",
					"    \"presentation_remote_updates_remote-closed\":               \"Document View End\",\n",
					"\n",
					"    \"auth/sessions_create\":                          \"Login to Docurated\",\n",
					"    \"space_create\":                                  \"Document created in docurated\",\n",
					"\n",
					"    \"view\":                                          \"View a document\",\n",
					"    \"preview\":                                       \"Preview a document\",\n",
					"    \"download\":                                      \"Download a document\",\n",
					"    \"modify\":                                        \"Modify a document\",\n",
					"    \"email\":                                         \"Email a document\"\n",
					"        }\n",
					"\n",
					"\n",
					"def get_sources():\n",
					"    return EVENT_TYPE_MAP.keys()\n",
					"\n",
					"def get_events_for_type(event_type, source=None):\n",
					"    output = set()\n",
					"    for e_source, vals in EVENT_TYPE_MAP.items():\n",
					"        if source is not None and e_source != source:\n",
					"            continue\n",
					"        for e_name, e_type in vals.items():\n",
					"            if type(event_type) == list:\n",
					"                if e_type in event_type:\n",
					"                    output.add(e_name)\n",
					"            else:\n",
					"                if e_type == event_type:\n",
					"                    output.add(e_name)\n",
					"    return list(output)\n",
					"\n",
					"# some python magic to flatMap the event names\n",
					"def get_event_names():\n",
					"    unflattened = [EVENT_TYPE_MAP[s].keys() for s in get_sources()]\n",
					"    return [i for sub in unflattened for i in sub]\n",
					"\n",
					"def get_event_type(source, event_name):\n",
					"    d = EVENT_TYPE_MAP.get(source, {})\n",
					"    return d.get(event_name)\n",
					"\n",
					"\n",
					"\n",
					"TIME_FMT_MS = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
					"TIME_FMT = \"yyyy-MM-dd HH:mm:ss\"\n",
					"FIELD_INT = 1\n",
					"FIELD_LONG = 2\n",
					"FIELD_FLOAT = 3\n",
					"FIELD_JSON = 4\n",
					"FIELD_STRING = 5\n",
					"FIELD_TIMESTAMP = 6\n",
					"FIELD_BOOLEAN = 7\n",
					"FIELD_DECIMAL = 8\n",
					"FIELD_STRING_ARRAY = 9\n",
					"\n",
					"SQL_TIME_FORMAT_MS = \"%Y-%m-%d %H:%M:%S.%f\"\n",
					"SQL_TIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
					"\n",
					"try:\n",
					"    FIELD_MAPPINGS = {\n",
					"        FIELD_INT: (['integer', 'smallint', 'smallserial', 'serial'], np.int32),\n",
					"        FIELD_LONG: (['bigint', 'bigserial'], np.int64),\n",
					"        FIELD_DECIMAL: (['decimal', 'numeric'], np.float64),\n",
					"        FIELD_FLOAT: (['real', 'double'], np.float64),\n",
					"        FIELD_JSON: (['json', 'jsonb'], object),\n",
					"        FIELD_TIMESTAMP: (['timestamp'], object, 'timestamp without time zone'),\n",
					"        FIELD_BOOLEAN: (['boolean'], np.int8),\n",
					"        FIELD_STRING: (['text'], object),\n",
					"        FIELD_STRING_ARRAY: (['text[]'], object)\n",
					"    }\n",
					"except:\n",
					"    FIELD_MAPPINGS = None\n",
					"\n",
					"def _make_sql_to_field_mappings(d):\n",
					"    if d is None:\n",
					"        return None\n",
					"    m = {}\n",
					"    for k, v in d.items():\n",
					"        for t in v[0]:\n",
					"            m[t] = k\n",
					"    return m\n",
					"\n",
					"SQL_TO_FIELD_MAPPINGS = _make_sql_to_field_mappings(FIELD_MAPPINGS)\n",
					"\n",
					"def _pd_type_from_field_type(f):\n",
					"    return FIELD_MAPPINGS.get(f, (None, object))[1]\n",
					"\n",
					"def _field_type_from_sql(sql_type):\n",
					"    return SQL_TO_FIELD_MAPPINGS.get(sql_type, FIELD_STRING)\n",
					"\n",
					"def sql_type_from_field(f):\n",
					"    val = FIELD_MAPPINGS.get(f, (['text'], object))\n",
					"    if len(val) == 3:\n",
					"        return val[2]\n",
					"    else:\n",
					"        return val[0][0]\n",
					"\n",
					"def parse_time_str(value):\n",
					"    if \".\" in value:\n",
					"        return datetime.strptime(value, SQL_TIME_FORMAT_MS)\n",
					"    else:\n",
					"        return datetime.strptime(value, SQL_TIME_FORMAT)\n",
					"\n",
					"BLOCK_SIZE = 200 * 1024 * 1024;\n",
					"\n",
					"def tuples_from_file(input_file, comparable_from_line, block_size=BLOCK_SIZE):\n",
					"    while True:\n",
					"        lines = [(comparable_from_line(l), l) for l in input_file.readlines(block_size)]\n",
					"        if lines == []:\n",
					"            input_file.close()\n",
					"            break\n",
					"        for line in lines:\n",
					"            yield line\n",
					"\n",
					"def _make_iters(input_file_name, comparable_from_line, block_size, temp_dir):\n",
					"    iters = []\n",
					"    total_num_blocks = (os.stat(input_file_name).st_size / block_size) + 1\n",
					"    iter_block_size = int(block_size / total_num_blocks)\n",
					"    with open(input_file_name, 'r') as f:\n",
					"        while True:\n",
					"            lines = [(comparable_from_line(l), l) for l in f.readlines(block_size)]\n",
					"            if lines == []:\n",
					"                break\n",
					"            lines.sort(key=lambda l: l[0])\n",
					"            temp_file = tempfile.TemporaryFile(dir=temp_dir, mode='w+')\n",
					"            for line in lines:\n",
					"                temp_file.write(line[1])\n",
					"            temp_file.seek(0)\n",
					"            iters.append(tuples_from_file(temp_file, comparable_from_line, iter_block_size))\n",
					"    return iters\n",
					"\n",
					"def extsort(file_name, comparable_from_line, block_size=BLOCK_SIZE, temp_dir=None):\n",
					"    iters = _make_iters(file_name, comparable_from_line, block_size, temp_dir)\n",
					"    output_file_name = file_name + \".out\"\n",
					"    try:\n",
					"        with open(output_file_name, 'w') as f:\n",
					"            for line_tuple in heapq.merge(*iters):\n",
					"                f.write(line_tuple[1])\n",
					"        os.rename(output_file_name, file_name)\n",
					"    finally:\n",
					"        if os.path.exists(output_file_name):\n",
					"            os.unlink(output_file_name)\n",
					"\n",
					"class Field:\n",
					"    def __init__(self, name, field_type, position, nullable=True):\n",
					"        self.name = name\n",
					"        self.field_type = field_type\n",
					"        self.position = position\n",
					"        self.nullable = nullable\n",
					"\n",
					"    def parse_value(self, value, parse_jsonb=True,\n",
					"            parse_time=True, parse_bool=True):\n",
					"        if value == \"\\\\N\":\n",
					"            return None\n",
					"        elif self.field_type == FIELD_INT or self.field_type == FIELD_LONG:\n",
					"            try:\n",
					"                return int(value)\n",
					"            except:\n",
					"                logging.info('Exception caused due to : '+ str(self.name) + ' ' + str(value) + ' ' + str(self.position) + ' ' + str(self.nullable))\n",
					"                return 0\n",
					"        elif self.field_type == FIELD_FLOAT or self.field_type == FIELD_DECIMAL:\n",
					"            return float(value)\n",
					"        elif self.field_type == FIELD_STRING_ARRAY:\n",
					"            updated_val = \"\"\n",
					"            try:\n",
					"                updated_val = ';'.join(eval(str(value)))\n",
					"            except:\n",
					"                updated_val = str(value)\n",
					"            return updated_val\n",
					"        elif parse_jsonb and self.field_type == FIELD_JSON:\n",
					"            return json.loads(value)\n",
					"        elif parse_time and self.field_type == FIELD_TIMESTAMP:\n",
					"            return parse_time_str(value)\n",
					"        elif parse_bool and self.field_type == FIELD_BOOLEAN:\n",
					"            v = value.lower()\n",
					"            return v == 't' or v == 'true'\n",
					"        else:\n",
					"            return codecs.escape_decode(\n",
					"                value.encode('utf-8'))[0].decode('utf-8')\n",
					"\n",
					"class Schema:\n",
					"    def __init__(self, fields):\n",
					"        self.fields = fields\n",
					"        self.field_map = dict((f.name, f) for f in fields)\n",
					"\n",
					"    def row_for_line(self, line, parse_jsonb=True,\n",
					"            parse_time=True, parse_bool=True):\n",
					"        obj = {}\n",
					"        values = line.rstrip(\"\\n\").split(\"\\t\")\n",
					"        for i, field in enumerate(self.fields):\n",
					"            obj[field.name] = field.parse_value(values[i], parse_jsonb,\n",
					"                    parse_time, parse_bool)\n",
					"        return obj\n",
					"\n",
					"    def create_comparable(self, *field_names):\n",
					"        fields = [self.field_map[f] for f in field_names]\n",
					"        def comparable(line):\n",
					"            values = line.rstrip(\"\\n\").split(\"\\t\")\n",
					"            return tuple((f.parse_value(values[f.position], False, False, False) or '') for f in fields)\n",
					"        return comparable\n",
					"\n",
					"    def to_json(self):\n",
					"        return json.dumps([[f.name, f.field_type, f.position, f.nullable] for f in self.fields])\n",
					"\n",
					"    def to_pg_cols(self):\n",
					"        cols = []\n",
					"        for f in self.fields:\n",
					"            c_type = sql_type_from_field(f.field_type)\n",
					"            c = \"%s %s\" % (f.name, c_type)\n",
					"            if not f.nullable:\n",
					"                c += ' NOT NULL'\n",
					"            cols.append(c)\n",
					"        return cols\n",
					"\n",
					"    @staticmethod\n",
					"    def from_json_file(fn):\n",
					"        with open(fn, 'r') as f:\n",
					"            return Schema.from_json(f.read())\n",
					"\n",
					"    @staticmethod\n",
					"    def from_json(json_str):\n",
					"        return Schema([Field(f[0], f[1], f[2], f[3]) for f in json.loads(json_str)])\n",
					"\n",
					"class FlatFile:\n",
					"    def __init__(self, fn, schema):\n",
					"        self.fn = fn\n",
					"        self.schema = schema\n",
					"\n",
					"    def iterate_row_lines(self):\n",
					"        with open(self.fn, 'r') as f:\n",
					"            for line in f:\n",
					"                yield line\n",
					"\n",
					"    def copy_row_lines(self, fn):\n",
					"        with open(fn, 'w') as f:\n",
					"            for line in self.iterate_row_lines():\n",
					"                f.write(line)\n",
					"\n",
					"    def iterate_rows(self, parse_jsonb=True, parse_time=True, parse_bool=True):\n",
					"        for line in self.iterate_row_lines():\n",
					"            yield self.schema.row_for_line(line, parse_jsonb,\n",
					"                    parse_time, parse_bool)\n",
					"\n",
					"    def output_sorted(self, output_fn, *columns, temp_dir=None):\n",
					"        comparable = self.schema.create_comparable(*columns)\n",
					"        self.copy_row_lines(output_fn)\n",
					"        extsort.extsort(output_fn, comparable, temp_dir=temp_dir)\n",
					"        return FlatFile(output_fn, self.schema)\n",
					"\n",
					"    def select(self, *field_names):\n",
					"        fields = [self.field_map[f] for f in field_names]\n",
					"        for line in self.iterate_row_lines():\n",
					"            values = line.rstrip(\"\\n\").split(\"\\t\")\n",
					"            yield tuple(values[f.position] for f in fields)\n",
					"\n",
					"    def select_to_file(self, file_name, *field_names):\n",
					"        fields = [self.schema.field_map[n] for n in field_names]\n",
					"        with open(file_name, 'r') as f:\n",
					"            for values in self.select(field_names):\n",
					"                f.write(\"\\t\".join(values))\n",
					"        return FlatFile(file_name, Schema(fields))\n",
					"\n",
					"    def partition_by_fields(self, field_names, output_dir, fn_template):\n",
					"        if isinstance(field_names, str):\n",
					"            field_names = [field_names]\n",
					"        fields = [self.schema.field_map[n] for n in field_names]\n",
					"        output_files = {}\n",
					"        try:\n",
					"            for line in self.iterate_row_lines():\n",
					"                values = line.rstrip(\"\\n\").split(\"\\t\")\n",
					"                key = tuple(f.parse_value(values[f.position]) for f in fields)\n",
					"                if key not in output_files:\n",
					"                    output_files[key] = open(\n",
					"                        os.path.join(output_dir, fn_template.format(*key)), 'w')\n",
					"                output_files[key].write(line)\n",
					"        finally:\n",
					"            for f in output_files.values():\n",
					"                f.close()\n",
					"\n",
					"    def to_dataframe(self):\n",
					"        def int_converter(x):\n",
					"            return -1 if x == '\\\\N' else int(x)\n",
					"        def bool_converter(x):\n",
					"            if x == '\\\\N':\n",
					"                return np.nan\n",
					"            return 1 if x in ['t', 'true'] else 0\n",
					"\n",
					"        converters = {}\n",
					"        for f in self.schema.fields:\n",
					"            if f.field_type == FIELD_BOOLEAN:\n",
					"                converters[f.name] = bool_converter\n",
					"            elif f.field_type == FIELD_INT or f.field_type == FIELD_LONG:\n",
					"                converters[f.name] = int_converter\n",
					"\n",
					"        if not os.path.exists(self.fn):\n",
					"            return pd.DataFrame(\n",
					"                columns=[f.name for f in self.schema.fields])\n",
					"        else:\n",
					"            return pd.read_table(\n",
					"                self.fn,\n",
					"                header=None,\n",
					"                names=[f.name for f in self.schema.fields],\n",
					"                converters=converters,\n",
					"                dtype=dict((f.name, _pd_type_from_field_type(f.field_type))\n",
					"                           for f in self.schema.fields),\n",
					"                na_values=[\"\\\\N\"])\n",
					"\n",
					"def get_col_type(c_num):\n",
					"    if c_num == FIELD_INT:\n",
					"        return IntegerType()\n",
					"    elif c_num == FIELD_LONG:\n",
					"        return LongType()\n",
					"    elif c_num == FIELD_FLOAT:\n",
					"        return FloatType()\n",
					"    elif c_num == FIELD_DECIMAL:\n",
					"        return DoubleType()\n",
					"    elif c_num == FIELD_BOOLEAN:\n",
					"        return BooleanType()\n",
					"    elif c_num == FIELD_STRING_ARRAY:\n",
					"        return StringType()\n",
					"    # elif c_num == flatutils.FIELD_TIMESTAMP:\n",
					"        # return TimestampType()\n",
					"    # elif c_num == flatutils.FIELD_JSON:\n",
					"    # elif c_num == flatutils.FIELD_STRING:\n",
					"    else:\n",
					"        return StringType()\n",
					"\n",
					"def parse_schema_path(spark, schema_path):\n",
					"    schema_text = spark.sparkContext.textFile(schema_path).collect()[0]\n",
					"    schema_json = json.loads(schema_text)\n",
					"    return [(f[0], get_col_type(f[1]), f[3]) for f in schema_json]\n",
					"\n",
					"def parse_line_wrapper(schema, parse_jsonb=False,\n",
					"        parse_time=False, parse_bool=False):\n",
					"    def parse_line(line):\n",
					"        return schema.row_for_line(line, parse_jsonb, parse_time, parse_bool)\n",
					"    return parse_line\n",
					"\n",
					"def load_dump_with_schema(spark, dump_path, schema_path, filter_method=None):\n",
					"    schema_text = spark.sparkContext.textFile(schema_path).collect()[0]\n",
					"    flat_schema = Schema.from_json(schema_text)\n",
					"\n",
					"    cols = parse_schema_path(spark, schema_path)\n",
					"    spark_schema = StructType([StructField(*c) for c in cols])\n",
					"\n",
					"    rdd = (spark.sparkContext.textFile(dump_path)\n",
					"        .filter(lambda l: len(l.split('\\t')) >= len(cols)))\n",
					"\n",
					"    if filter_method is not None:\n",
					"        rdd = rdd.filter(filter_method)\n",
					"\n",
					"    rdd = rdd.map(parse_line_wrapper(flat_schema, False, False, True))\n",
					"    return spark.createDataFrame(rdd, spark_schema)\n",
					"\n",
					"\n",
					"def load_dump(spark, dump_dir, table_name, filter_method=None):\n",
					"    dump_path = os.path.join(dump_dir, '%s.dump' % table_name)\n",
					"    schema_path = os.path.join(dump_dir, '%s.schema' % table_name)\n",
					"    return load_dump_with_schema(spark, dump_path, schema_path, filter_method)\n",
					"def load_dimension(spark, dump_dir, table_name, cols=[]):\n",
					"    df = load_dump(spark, dump_dir, table_name)\n",
					"    if len(cols) > 0:\n",
					"        df = df.select(*cols)\n",
					"    return df.withColumn(\"skey\", monotonically_increasing_id())\n",
					"\n",
					"def is_string_array(c_num):\n",
					"    if c_num == FIELD_STRING_ARRAY:\n",
					"        return True\n",
					"    else:\n",
					"        return False\n",
					"\n",
					"def _check_and_get_json(string):\n",
					"    try:\n",
					"        js = json.loads(string)\n",
					"    except ValueError as e:\n",
					"        return False\n",
					"    return js\n",
					"\n",
					"def get_array_fields(spark, dump_dir, schema_name):\n",
					"    schema_path = os.path.join(dump_dir, schema_name)\n",
					"    schema_text = spark.sparkContext.textFile(schema_path).collect()[0]\n",
					"    schema_json = json.loads(schema_text)\n",
					"    string_arr_fields = []\n",
					"    for f in schema_json:\n",
					"        if (is_string_array(f[1])):\n",
					"            string_arr_fields.append(f[0])\n",
					"    return string_arr_fields\n",
					"\n",
					"def organization_dim(spark, dump_dir):\n",
					"    return load_dimension(spark, dump_dir, 'organizations', ['id', 'name'])\n",
					"\n",
					"def user_dim(spark, dump_dir):\n",
					"    usersDF = load_dimension(spark, dump_dir, 'users',\n",
					"                             ['id', 'organization_id', 'email', 'first_name',\n",
					"                              'last_name', 'is_active', 'email_domain'])\n",
					"    return (usersDF.withColumn(\n",
					"        'user_name',\n",
					"        when(\n",
					"            (usersDF.first_name.isNull() & usersDF.last_name.isNull()) |\n",
					"            (\n",
					"                expr(\"length(first_name) = 0\") &\n",
					"                expr(\"length(last_name) = 0\")\n",
					"            ),\n",
					"            usersDF.email\n",
					"        )\n",
					"        .otherwise(\n",
					"            concat_ws(' - ',\n",
					"                      concat_ws(' ', 'first_name', 'last_name'),\n",
					"                      'email')\n",
					"        )\n",
					"    ))\n",
					"\n",
					"def group_dim(spark, dump_dir):\n",
					"    df = load_dimension(\n",
					"        spark, dump_dir, 'groups',\n",
					"        ['id', 'name', 'organization_id', 'source', 'source_id'])\n",
					"    null_opts = {\n",
					"        'id': -1, 'skey': -1, 'name': 'No Group', 'organization_id': -1}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    return (\n",
					"        df\n",
					"        .union(null_row)\n",
					"        .withColumn('group_source_name', concat_ws(': ', 'source', 'name')))\n",
					"\n",
					"def group_users_dim(spark, dump_dir, groupDim, userDim):\n",
					"    guDim = load_dimension(spark, dump_dir, 'group_users', ['id', 'group_id',\n",
					"                                                            'user_id'])\n",
					"    gDim = (groupDim\n",
					"            .select(\n",
					"                col('skey').alias('group_skey'),\n",
					"                col('id').alias('group_id')))\n",
					"    uDim = (userDim\n",
					"            .select(\n",
					"                col('skey').alias('user_skey'),\n",
					"                col('id').alias('user_id'),\n",
					"                'organization_id'))\n",
					"    df = (guDim\n",
					"          .join(gDim, 'group_id')\n",
					"          .join(uDim, 'user_id')\n",
					"          .select('id', 'skey', 'group_skey', 'user_skey', 'group_id',\n",
					"                  'user_id', uDim.organization_id))\n",
					"    df.cache()\n",
					"    group_keys = (df\n",
					"                  .select('user_skey')\n",
					"                  .distinct()\n",
					"                  .withColumn(\"group_users_gkey\",\n",
					"                              monotonically_increasing_id()))\n",
					"    group_keys.cache()\n",
					"    df = df.join(group_keys, 'user_skey')\n",
					"    null_opts = {\n",
					"            'id': -1, 'skey': -1, 'group_skey': -1, 'group_id': -1,\n",
					"            'user_id': -1, 'user_skey': -1, 'organization_id': -1,\n",
					"            'group_users_gkey': -1}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    df = df.union(null_row)\n",
					"    user_bridge = (df\n",
					"                   .select(col('user_skey').alias('skey'), 'group_users_gkey')\n",
					"                   .distinct())\n",
					"    userDim = (userDim\n",
					"               .join(user_bridge, 'skey', 'left')\n",
					"               .na.fill({'group_users_gkey': -1}))\n",
					"    return [df, userDim]\n",
					"\n",
					"def event_name_dim(spark):\n",
					"    schema = StructType([\n",
					"        StructField(\"name\", StringType(), False),\n",
					"        StructField(\"type\", StringType(), False),\n",
					"        StructField(\"type_str\", StringType(), False),\n",
					"        StructField(\"name_str\", StringType(), False)])\n",
					"\n",
					"    arr = []\n",
					"    sources = EVENT_TYPE_MAP.keys()\n",
					"    for s in sources:\n",
					"        for e_name, e_type in EVENT_TYPE_MAP[s].items():\n",
					"            row = {'name': e_name, 'type': e_type,\n",
					"                   'type_str': TYPES_STR_MAP[e_type],\n",
					"                   'name_str': VALUES_STR_MAP[e_name]}\n",
					"            arr.append(row)\n",
					"    return (spark\n",
					"            .createDataFrame(arr, schema)\n",
					"            .distinct()\n",
					"            .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"\n",
					"def source_dim(spark):\n",
					"    schema = StructType([\n",
					"        StructField(\"name\", StringType(), False),\n",
					"        StructField(\"source_str\", StringType(), False)])\n",
					"\n",
					"    arr = []\n",
					"    for s in EVENT_TYPE_MAP.keys():\n",
					"        arr.append({'name': s, 'source_str': SOURCES_STR_MAP[s]})\n",
					"    return (spark\n",
					"            .createDataFrame(arr, schema)\n",
					"            .distinct()\n",
					"            .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"\n",
					"def space_dim(spark, dump_dir):\n",
					"    df = load_dimension(\n",
					"        spark, dump_dir, 'spaces',\n",
					"        [\"id\", \"name\", \"size\", \"user_id\", \"visible\", \"upload_source\",\n",
					"         \"organization_id\", \"modified_at\", \"source_file_type\",\n",
					"         \"version_series_id\", \"underlying_version_label\",\n",
					"         \"docurated_version_label\", \"content_identifier\", \"space_key\",\n",
					"         \"path\", \"inserted_at\", \"created_at\"])\n",
					"    null_opts = {\n",
					"            'id': -1, 'skey': -1, 'name': 'No Space', 'space_key': 'no_space',\n",
					"            'organization_id': -1, 'visible': False, 'path': 'no_path', 'created_at': '1970-01-01 00:00:00'}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    return df.union(null_row)\n",
					"\n",
					"def topic_dim(spark, dump_dir):\n",
					"    df = load_dimension(spark, dump_dir, 'topics', ['id', 'name',\n",
					"                                                    'organization_id'])\n",
					"    null_opts = {\n",
					"            'id': -1, 'skey': -1, 'name': 'No Topic', 'organization_id': -1,\n",
					"            'display_name': 'No Topic', 'custom_display_name': 'No Topic'}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    return df.union(null_row)\n",
					"\n",
					"def collection_dim(spark, dump_dir):\n",
					"    df = load_dimension(spark, dump_dir, 'collections', ['id', 'name','organization_id', 'user_id'])\n",
					"    null_opts = {'id': -1, 'skey': -1, 'name': 'No Collection', 'organization_id': -1, 'user_id': -1}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    return df.union(null_row)\n",
					"\n",
					"def components_dim(spark, dump_dir):\n",
					"    def clean_title(title):\n",
					"        clean_title = _check_and_get_json(title)\n",
					"        if clean_title and type(clean_title)==dict:\n",
					"            logging.info(\"Found Junk Title {0}\".format(clean_title.get(\"content\")))\n",
					"            title_processed = clean_title.get(\"content\", [\"INVALID INGESSION\"])\n",
					"            if type(title_processed)==list:\n",
					"                return title_processed[0]\n",
					"            else:\n",
					"                return title_processed\n",
					"        return title\n",
					"\n",
					"    df = (load_dimension(spark, dump_dir, 'components',\n",
					"                        ['id', 'upstream_section_id', 'title',\n",
					"                        'version', 'organization_id',\n",
					"                        'version_series_id'])\n",
					"                        .withColumn('title_clean',\n",
					"                                    udf(lambda x: clean_title(x))(\"title\"))\n",
					"                        .drop('title')\n",
					"                        .withColumnRenamed('title_clean', 'title'))\n",
					"\n",
					"    null_opts = {'id': -1, 'upstream_section_id': 'null_upstream_id', 'title': 'No title',\n",
					"    'version': 'No version', 'organization_id': -1, 'version_series_id': 'null_version_id',\n",
					"    'skey': -1}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    return (df\n",
					"            .union(null_row)\n",
					"            .select(\n",
					"                'id',\n",
					"                'upstream_section_id',\n",
					"                substring(col('title'), 0, 200).alias('title'),\n",
					"                'version',\n",
					"                'organization_id',\n",
					"                'version_series_id',\n",
					"                'skey'\n",
					"            ))\n",
					"\n",
					"def spaces_topics_dim(spark, dump_dir, spaceDim, topicDim):\n",
					"    stDim = load_dimension(\n",
					"        spark, dump_dir, 'spaces_topics', ['id', 'space_id', 'topic_id'])\n",
					"    sDim = (spaceDim\n",
					"            .select(\n",
					"                col('skey').alias('space_skey'),\n",
					"                col('id').alias('space_id')))\n",
					"    tDim = (topicDim\n",
					"            .select(\n",
					"                col('skey').alias('topic_skey'),\n",
					"                col('id').alias('topic_id'),\n",
					"                'organization_id'))\n",
					"    df = (stDim\n",
					"          .join(sDim, 'space_id')\n",
					"          .join(tDim, 'topic_id')\n",
					"          .select('id', 'skey', 'space_skey', 'topic_skey', 'space_id',\n",
					"                  'topic_id', tDim.organization_id))\n",
					"    df.cache()\n",
					"    group_keys = (\n",
					"        df\n",
					"        .select('space_skey')\n",
					"        .distinct()\n",
					"        .withColumn(\"spaces_topics_gkey\", monotonically_increasing_id()))\n",
					"    group_keys.cache()\n",
					"    df = df.join(group_keys, 'space_skey')\n",
					"    null_opts = {\n",
					"        'id': -1, 'skey': -1, 'space_skey': -1, 'space_id': -1,\n",
					"        'topic_id': -1, 'topic_skey': -1, 'organization_id': -1,\n",
					"        'spaces_topics_gkey': -1}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    df = df.union(null_row)\n",
					"    space_bridge = (\n",
					"        df\n",
					"        .select(col('space_skey').alias('skey'), 'spaces_topics_gkey')\n",
					"        .distinct())\n",
					"    spaceDim = (spaceDim\n",
					"                .join(space_bridge, 'skey', 'left')\n",
					"                .na.fill({'spaces_topics_gkey': -1}))\n",
					"    return [df, spaceDim]\n",
					"\n",
					"def session_time_view(spark, dump_dir):\n",
					"    \"\"\"\n",
					"    This is to traverse the sessions data for\n",
					"    utilizing presenter custom events.\n",
					"    \"\"\"\n",
					"    df = load_dimension(spark, dump_dir, 'presentation_sessions')\n",
					"    return (df\n",
					"            .withColumn('total_time',\n",
					"                        udf(lambda x: json.loads(x)['total_time'])(\"data\"))\n",
					"            .select('key',\n",
					"            'link_id',\n",
					"            'space_id',\n",
					"            'organization_id',\n",
					"            'start_time',\n",
					"            'user_id',\n",
					"            lower(col('guest')).alias(\"guest\"),\n",
					"            col('total_time').cast('long')\n",
					"           ))\n",
					"\n",
					"def collections_items_dim(spark, dump_dir, spaceDim, collectionDim):\n",
					"    ciDim = load_dimension(\n",
					"        spark, dump_dir, 'collections_items', ['id', 'item_id', 'item_type', 'collection_id'])\n",
					"    sDim = (spaceDim\n",
					"            .select(\n",
					"                col('skey').alias('space_skey'),\n",
					"                col('id').alias('item_id')))\n",
					"    cDim = (collectionDim\n",
					"            .select(\n",
					"                col('skey').alias('collection_skey'),\n",
					"                col('id').alias('collection_id'),\n",
					"                'organization_id'))\n",
					"    df = (ciDim\n",
					"          .join(sDim, 'item_id')\n",
					"          .join(cDim, 'collection_id')\n",
					"          .select('id', 'skey', 'space_skey', 'collection_skey', 'item_id', 'item_type',\n",
					"                  'collection_id', cDim.organization_id))\n",
					"    df.cache()\n",
					"    group_keys = (\n",
					"        df\n",
					"        .select('space_skey')\n",
					"        .distinct()\n",
					"        .withColumn(\"spaces_collections_gkey\", monotonically_increasing_id()))\n",
					"    group_keys.cache()\n",
					"    df = df.join(group_keys, 'space_skey')\n",
					"    null_opts = {\n",
					"        'id': -1, 'skey': -1, 'space_skey': -1, 'item_id': -1, 'item_type': 'N/A',\n",
					"        'collection_id': -1, 'collection_skey': -1, 'organization_id': -1,\n",
					"        'spaces_collections_gkey': -1}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    df = df.union(null_row)\n",
					"    space_bridge = (\n",
					"        df\n",
					"        .select(col('space_skey').alias('skey'), 'spaces_collections_gkey')\n",
					"        .distinct())\n",
					"    spaceDim = (spaceDim\n",
					"                .join(space_bridge, 'skey', 'left')\n",
					"                .na.fill({'spaces_collections_gkey': -1}))\n",
					"    return df\n",
					"\n",
					"def space_extras_dim(spark, dump_dir, space_dim):\n",
					"    df = load_dimension(\n",
					"        spark, dump_dir, 'space_extras',\n",
					"        [\"id\", \"space_id\", \"organization_id\", \"description\"])\n",
					"    null_opts = {\n",
					"            'id': -1, 'skey': -1, 'space_id': -1, 'organization_id': -1, 'description': ''}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    space_extra_dim = df.union(null_row)\n",
					"    mod_sp_dim = (space_dim.select('skey', 'id', 'organization_id')\n",
					"                    .withColumnRenamed('skey', 'space_skey')\n",
					"                    .withColumnRenamed('id', 'space_id'))\n",
					"    return (space_extra_dim.join(mod_sp_dim, ['organization_id', 'space_id'], 'left')\n",
					"                            .distinct()\n",
					"                            .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"\n",
					"def items_components_dim(spark, dump_dir, spaceDim, componentsDim):\n",
					"    # TODO: Ideally this cleaning should be done at the source\n",
					"    # while ingesting metadata for all components. This is to\n",
					"    # save overflowing of string in redshift columns.\n",
					"\n",
					"    def clean_metadta_json(meta_json):\n",
					"        try:\n",
					"            metadata = json.loads(meta_json)\n",
					"            metadata_clean = {}\n",
					"            for k, v in metadata.items():\n",
					"                if k not in ['title', 'type', 'section-xpath'] and v not in [None, []]:\n",
					"                    metadata_clean[k] = v\n",
					"            return json.dumps(metadata_clean)\n",
					"        except:\n",
					"            return '{}'\n",
					"\n",
					"    icDim = load_dimension(spark, dump_dir, 'items_components',\n",
					"                        ['id', 'component_id', 'space_id', 'parent_id', 'parent_type','position',\n",
					"                        'tile_start', 'tile_end', 'upstream_root_component_id', 'metadata', 'upstream_ref_component_id'])\n",
					"\n",
					"    icDim = (icDim\n",
					"             .withColumn('metadata_clean', udf(lambda x: clean_metadta_json(x))(\"metadata\"))\n",
					"             .drop('metadata')\n",
					"             .withColumnRenamed('metadata_clean', 'metadata'))\n",
					"    sDim = (spaceDim\n",
					"            .select(\n",
					"                'version_series_id',\n",
					"                col('skey').alias('space_skey'),\n",
					"                col('id').alias('space_id')))\n",
					"    cDim = (componentsDim\n",
					"            .select(\n",
					"                col('skey').alias('component_skey'),\n",
					"                col('id').alias('component_id'),\n",
					"                'organization_id'))\n",
					"    df = (icDim\n",
					"            .join(sDim, 'space_id')\n",
					"            .join(cDim, 'component_id')\n",
					"            .select('id', 'skey', 'space_skey', 'space_id', 'component_skey',\n",
					"            'component_id','upstream_root_component_id' , cDim.organization_id,\n",
					"            'version_series_id', 'metadata', 'position', 'tile_start',\n",
					"            'tile_end', 'parent_id', 'parent_type', 'upstream_ref_component_id'))\n",
					"    df.cache()\n",
					"    null_opt = {'id': -1, 'component_id': -1, 'space_id': -1, 'parent_id': -1,\n",
					"                 'parent_type': 'No parent type', 'position': -1, 'tile_start': -1, 'version_series_id': 'null_vsid',\n",
					"                 'tile_end': -1, 'upstream_root_component_id': 'null_root_id', 'upstream_ref_component_id': 'null_ref_id', 'metadata': \"{}\",\n",
					"                 'space_skey': -1, 'component_skey': -1, 'organization_id': -1, 'skey': -1}\n",
					"    null_row = spark.createDataFrame([null_opt], schema=df.schema)\n",
					"    return df.union(null_row)\n",
					"\n",
					"def add_opp_dates(opps, opp_history, min_window_days=60):\n",
					"    opp_hist_created = (opp_history\n",
					"                        .select('organization_id', 'opportunity_id',\n",
					"                                'created_date')\n",
					"                        .withColumn('created_date_secs',\n",
					"                                    unix_timestamp('created_date',\n",
					"                                                   TIME_FMT_MS)))\n",
					"    ends = (opp_hist_created\n",
					"            .groupBy('organization_id', 'opportunity_id')\n",
					"            .agg(max('created_date_secs').alias('created_date_secs'))\n",
					"            .join(opp_hist_created,\n",
					"                  ['organization_id', 'opportunity_id', 'created_date_secs'])\n",
					"            .select(\n",
					"                col('opportunity_id').alias('salesforce_id'),\n",
					"                col('created_date').alias('eff_date_end')))\n",
					"    return (opps\n",
					"            .join(ends, 'salesforce_id', 'left')\n",
					"            .withColumn(\n",
					"                'effective_closed',\n",
					"                when(col('is_closed'), col('eff_date_end'))\n",
					"                .otherwise(current_timestamp()))\n",
					"            .withColumn('min_window_created',\n",
					"                        date_sub('eff_date_end', min_window_days))\n",
					"            .withColumn('effective_created', date_format('created_date', TIME_FMT))\n",
					"            .drop('eff_date_end', 'min_window_created'))\n",
					"\n",
					"def sf_opportunity_dim(spark, dump_dir):\n",
					"    df = load_dimension(spark, dump_dir, 'salesforce_opps')\n",
					"    return (\n",
					"        df\n",
					"        .withColumn('won_amount',\n",
					"                    when(expr('is_closed = TRUE AND is_won = TRUE'),\n",
					"                         col('amount'))\n",
					"                    .otherwise(0.0).cast('double'))\n",
					"        .withColumn('created_date_skey',\n",
					"                    unix_timestamp('created_date',\n",
					"                                   TIME_FMT_MS).cast('timestamp')))\n",
					"\n",
					"def sf_account_dim(spark, dump_dir):\n",
					"    return load_dimension(spark, dump_dir, 'salesforce_accounts')\n",
					"\n",
					"def sf_contact_dim(spark, dump_dir):\n",
					"    return load_dimension(spark, dump_dir, 'salesforce_contacts')\n",
					"\n",
					"\n",
					"def sf_opp_history_dim(spark, dump_dir):\n",
					"    return load_dimension(spark, dump_dir, 'salesforce_opphists')\n",
					"\n",
					"\n",
					"def sf_user_dim(spark, dump_dir):\n",
					"    return load_dimension(spark, dump_dir, 'salesforce_users')\n",
					"\n",
					"def sf_oppcontrols_dim(spark, dump_dir):\n",
					"    return load_dimension(spark, dump_dir, 'salesforce_oppcontroles')\n",
					"\n",
					"def sf_rep_dim(dims,\n",
					"               all_time_start=datetime.now()-timedelta(days=730),\n",
					"               win_time_start=datetime.now()-timedelta(days=180)):\n",
					"    opps = dims['sf_opportunities']\n",
					"    opp_history = dims['sf_opp_histories']\n",
					"    users = (dims['users']\n",
					"             .select(\n",
					"                 col('skey').alias('user_skey'),\n",
					"                 'organization_id',\n",
					"                 'email'))\n",
					"    sf_users = (dims['sf_users']\n",
					"                .filter('is_active = TRUE')\n",
					"                .join(users, ['organization_id', 'email'])\n",
					"                .select(\n",
					"                    'organization_id',\n",
					"                    'user_skey',\n",
					"                    col('salesforce_id').alias('owner_id')))\n",
					"\n",
					"    opps_with_dates = (add_opp_dates(opps, opp_history)\n",
					"                       .select(\n",
					"                           'organization_id', 'salesforce_id', 'owner_id',\n",
					"                           'effective_created', 'is_won'))\n",
					"    opps_with_dates.cache()\n",
					"\n",
					"    won_opps = (opps_with_dates\n",
					"                .filter(\"effective_created >= '%s'\" % win_time_start)\n",
					"                .withColumn('success',\n",
					"                            when(expr('is_won = TRUE'), 1)\n",
					"                            .otherwise(0))\n",
					"                .select('organization_id', 'salesforce_id', 'success'))\n",
					"\n",
					"    df = (opps_with_dates\n",
					"          .filter(\"effective_created >= '%s'\" % all_time_start)\n",
					"          .join(sf_users, ['organization_id', 'owner_id'])\n",
					"          .join(won_opps, ['organization_id', 'salesforce_id'])\n",
					"          .groupBy('organization_id', 'owner_id')\n",
					"          .sum('success')\n",
					"          .select(\n",
					"              'organization_id',\n",
					"              'owner_id',\n",
					"              col('sum(success)').alias('success_count')))\n",
					"    max_counts = (\n",
					"        df\n",
					"        .groupBy('organization_id')\n",
					"        .max('success_count')\n",
					"        .withColumnRenamed('max(success_count)', 'max_success_count'))\n",
					"    return (\n",
					"        df\n",
					"        .join(sf_users, ['organization_id', 'owner_id'])\n",
					"        .join(max_counts, 'organization_id')\n",
					"        .withColumn('percentile',\n",
					"                    100*col('success_count')/col('max_success_count'))\n",
					"        .withColumn('is_top', when(col('percentile') >= 0.8, col('owner_id'))\n",
					"                    .otherwise(None))\n",
					"        .drop('max_success_count')\n",
					"        .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"\n",
					"def tableau_user_dim(spark, dump_dir):\n",
					"    return load_dimension(spark, dump_dir, 'tableau_users')\n",
					"\n",
					"def content_score_dim(spark, dump_dir):\n",
					"    df = load_dimension(\n",
					"        spark, dump_dir, 'content_scores',\n",
					"        [\"id\", \"org_id\", \"user_id\", \"shares\", \"views\", \"usage\",\n",
					"         \"content_score\", \"month\", \"year\"])\n",
					"    null_opts = {\n",
					"            'id': -1, 'skey': -1, 'org-id': -1, 'user_id': -1, 'shares': -1,'views': -1,\n",
					"            'usage': -1, 'content_score': -1, 'month': -1, 'year': -1}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    return df.union(null_row)\n",
					"\n",
					"def sync_metadata_dim(spark, dump_dir, space_dim, sync_org):\n",
					"    logging.info(\"LOAD DIMENTION DIR: >> \" + 'sync_metadata_dim' + str(sync_org))\n",
					"    sync_metadata_df = load_dimension(spark, dump_dir, 'sync_metadata_' + str(sync_org))\n",
					"    sDim = (space_dim\n",
					"            .select(\n",
					"                col('skey').alias('space_skey'),\n",
					"                col('id').alias('space_id'),\n",
					"                'organization_id'))\n",
					"    string_arr_fields = get_array_fields(spark, dump_dir, 'sync_metadata_' + str(sync_org) + \".schema\")\n",
					"    logging.info(\"SCHEMA DIR: >> \" + 'sync_metadata_' + str(sync_org) + \".schema\")\n",
					"    for arr_field in string_arr_fields:\n",
					"        logging.info(\"ARRAY FIELD TO CONVERT >> \" + arr_field)\n",
					"        # sync_metadata_df = (sync_metadata_df.withColumn(arr_field + \"_mod\" , concat_ws(',', arr_field)).drop(arr_field).withColumnRenamed(arr_field + \"_mod\", arr_field))\n",
					"\n",
					"    return sync_metadata_df.join(sDim, ['organization_id', 'space_id'], 'left')\n",
					"\n",
					"def process_cep_sources_dim(dim_with_root_asset, spaceDim):\n",
					"    sDim = (spaceDim\n",
					"            .select('skey',\n",
					"                     col('id').alias('s_id'),\n",
					"                     col('name').alias('s_name')))\n",
					"    return (dim_with_root_asset\n",
					"                .na.fill({\n",
					"                    \"source_asset_name\": \"\",\n",
					"                    \"source_asset_id\": \"\"\n",
					"                })\n",
					"                .join(sDim, dim_with_root_asset.space_skey == sDim.skey, 'left')\n",
					"                .withColumn('source_asset_id',\n",
					"                             when(col('source_asset_id') == \"\", col('s_id'))\n",
					"                                 .otherwise(col('source_asset_id')))\n",
					"                .withColumn('source_asset_name',\n",
					"                             when(col('source_asset_name') == \"\", col('s_name'))\n",
					"                                 .otherwise(col('source_asset_name')))\n",
					"                .select(\n",
					"                    'space_skey',\n",
					"                    'space_id',\n",
					"                    'organization_id',\n",
					"                    'source_asset_id',\n",
					"                    'source_asset_name'))\n",
					"\n",
					"def event_dim(spark, dump_dir):\n",
					"    return load_dimension(spark, dump_dir, 'events')\n",
					"\n",
					"\n",
					"import functools\n",
					"import numbers\n",
					"\n",
					"class EmailDocument:\n",
					"    \"\"\"\n",
					"    This is for documents sent via email, that will *ALWAYS* have\n",
					"    both a version_series_id and docurated_version_label which can\n",
					"    be used to uniquely identify a space.\n",
					"\n",
					"    Args:\n",
					"        version_series_id (string): this is obvious\n",
					"        docurated_version_label (string): this is obvious\n",
					"    \"\"\"\n",
					"    __slots__ = [\"version_series_id\", \"docurated_version_label\"]\n",
					"\n",
					"    def __init__(self, version_series_id, docurated_version_label):\n",
					"        self.version_series_id = version_series_id\n",
					"        self.docurated_version_label = docurated_version_label\n",
					"\n",
					"class SpaceDocument:\n",
					"    \"\"\"\n",
					"    This is for documents coming from repos where we have a space_id,\n",
					"    like website.\n",
					"\n",
					"    Args:\n",
					"        space_id (integer): this is obvious\n",
					"    \"\"\"\n",
					"    __slots__ = [\"space_id\"]\n",
					"\n",
					"    def __init__(self, space_id):\n",
					"        self.space_id = space_id\n",
					"\n",
					"class RepoDocument:\n",
					"    \"\"\"\n",
					"    This is for documents coming from repos where we dont have a\n",
					"    space_id, we only have a version_series_id and some form of date\n",
					"    that we use to match with the appropriate space. For some sources\n",
					"    this date is missing, so we must resort to simply using the \"time\"\n",
					"    of the event (such as SharePoint).\n",
					"\n",
					"    Args:\n",
					"        version_series_id (string): this is obvious\n",
					"        modified_at (string): *this is a string*\n",
					"    \"\"\"\n",
					"    __slots__ = [\"version_series_id\", \"modified_at\"]\n",
					"\n",
					"    def __init__(self, version_series_id, modified_at):\n",
					"        self.version_series_id = version_series_id\n",
					"        self.modified_at = modified_at\n",
					"\n",
					"class EmailRecipient:\n",
					"    \"\"\"\n",
					"    This is for pretty self explanatory.\n",
					"    \"\"\"\n",
					"    __slots__ = [\"email\", \"domain\"]\n",
					"\n",
					"    def __init__(self, email):\n",
					"        self.email = email.lower().strip()\n",
					"        self.domain = extract_domain(email)\n",
					"\n",
					"class ParsedEmail:\n",
					"    \"\"\"\n",
					"    This represents a parsed email event, extracting documents/\n",
					"    recipients/links/etc.\n",
					"\n",
					"    Args:\n",
					"        sender (string): this is obvious\n",
					"        to/cc (string[]): this is obvious\n",
					"        recipients (EmailRecipient[]): this is obvious\n",
					"        documents (EmailDocument[]): documents attached to the email\n",
					"        *_links (string[]): various forms of links present in the email\n",
					"    \"\"\"\n",
					"    __slots__ = [\"sender\", \"to\", \"cc\", \"recipients\", \"documents\",\n",
					"                 \"bitly_links\", \"box_links\", \"gdrive_links\",\n",
					"                 \"docurated_links\", \"dropbox_links\"]\n",
					"\n",
					"    def __init__(self, sender, to, cc, documents,\n",
					"                 bitly_links, box_links, gdrive_links,\n",
					"                 docurated_links, dropbox_links):\n",
					"        self.sender = sender.lower()\n",
					"        self.to = to\n",
					"        self.cc = cc\n",
					"        self.recipients = to + cc\n",
					"        self.documents = documents\n",
					"        self.bitly_links = bitly_links\n",
					"        self.box_links = box_links\n",
					"        self.gdrive_links = gdrive_links\n",
					"        self.docurated_links = docurated_links\n",
					"        self.dropbox_links = dropbox_links\n",
					"\n",
					"    def domains_json(self):\n",
					"        domains = list(set([r.domain for r in self.recipients]))\n",
					"        return json.dumps(domains)\n",
					"\n",
					"    def recipients_json(self):\n",
					"        emails = list(set([r.email for r in self.recipients]))\n",
					"        return json.dumps(emails)\n",
					"\n",
					"class ParsedPresentation:\n",
					"    \"\"\"\n",
					"    This represents a parsed presentation event, extracting the\n",
					"    email of the *presentation creator*, presentation event\n",
					"    type (user-joins, slide-change, etc.), documents (SpaceDocuments),\n",
					"    and recipients (it was sent to that are performing the actions).\n",
					"    \"\"\"\n",
					"    __slots__ = ['user_id', 'email', 'event', 'recipients', 'documents', 'collections']\n",
					"\n",
					"    def __init__(self, user_id, email, event, recipients, documents, collections):\n",
					"        self.user_id = user_id\n",
					"        self.email = email.lower() if email is not None else None\n",
					"        self.event = event\n",
					"        self.recipients = recipients\n",
					"        self.documents = documents\n",
					"        self.collections = collections\n",
					"        if not isinstance(self.collections, list):\n",
					"            self.collections = [self.collections]\n",
					"\n",
					"    def recipients_json(self):\n",
					"        emails = list(set([r.email for r in self.recipients]))\n",
					"        return json.dumps(emails)\n",
					"\n",
					"class ParsedWebsiteEvent:\n",
					"    \"\"\"\n",
					"    This represents all events that come from website (Docurated).\n",
					"    user_id is the user_id of the docurated user, and documents are always\n",
					"    SpaceDocuments (if applicable). Topics are just topic_ids (if applicable).\n",
					"    \"email\" is only included to be backwards compatible with\n",
					"    unnest_documents in the OTP DAG.\n",
					"    \"\"\"\n",
					"    __slots__ = [\"user_id\", \"email\", \"documents\", \"topics\", \"collections\"]\n",
					"\n",
					"    def __init__(self, user_id, email=None, documents=[], topics=[], collections=[]):\n",
					"        self.user_id = user_id\n",
					"        self.email = email\n",
					"        self.documents = documents\n",
					"        if not isinstance(self.documents, list):\n",
					"            self.documents = [self.documents]\n",
					"        self.topics = topics\n",
					"        if not isinstance(self.topics, list):\n",
					"            self.topics = [self.topics]\n",
					"        self.collections = collections\n",
					"        if not isinstance(self.collections, list):\n",
					"            self.collections = [self.collections]\n",
					"\n",
					"class ParsedSearchesRecord(ParsedWebsiteEvent):\n",
					"    \"\"\"\n",
					"    This representes website searches mapped with the\n",
					"    respective space id and helps in getting answers like\n",
					"    what keywords leads to this space in search results.\n",
					"    search_records is the param added extra in this.\n",
					"    \"\"\"\n",
					"    __slots__= [\"user_id\", \"email\", \"documents\", \"topics\",\n",
					"                \"collections\", \"search_records\"]\n",
					"\n",
					"    def __init__(self, user_id, email=None, documents=[],\n",
					"                 topics=[], collections=[], search_records=[]):\n",
					"        super().__init__(user_id, email, documents, topics, collections)\n",
					"        self.search_records=search_records\n",
					"\n",
					"class ParsedRepoEvent:\n",
					"    \"\"\"\n",
					"    This represents all other events that dont fall into either\n",
					"    Emails, Presentations, or Website categories. Email is the\n",
					"    email of the event actor, and documents can be either\n",
					"    SpaceDocuments or RepoDocuments.\n",
					"    \"\"\"\n",
					"    __slots__ = [\"email\", \"documents\"]\n",
					"\n",
					"    def __init__(self, email, documents=[]):\n",
					"        self.email = email.lower().strip()\n",
					"        self.documents = documents\n",
					"        if not isinstance(self.documents, list):\n",
					"            self.documents = [self.documents]\n",
					"\n",
					"def extract_domain(email):\n",
					"    try:\n",
					"        d = email.split(\"@\", 2)[1].lower()\n",
					"        return d if d != '' else None\n",
					"    except:\n",
					"        return None\n",
					"\n",
					"def _parse_user_name(data_obj):\n",
					"    if 'user_email' in data_obj:\n",
					"        return data_obj['user_email']\n",
					"\n",
					"    user_name = data_obj.get('user_name')\n",
					"    if user_name is None or user_name == '<no user>':\n",
					"        return None\n",
					"\n",
					"    matches = re.match(r'.*<(.*)>', user_name)\n",
					"    if matches is None:\n",
					"        return None\n",
					"    return matches.group(1)\n",
					"\n",
					"def _parse_time(time):\n",
					"    if isinstance(time, numbers.Number):\n",
					"        return str(datetime.datetime.utcfromtimestamp(time))\n",
					"    else:\n",
					"        return time\n",
					"\n",
					"def parse_data(json_str):\n",
					"    \"\"\"\n",
					"    Given the string content of the `data` field in the events table,\n",
					"    returns a Python type with the json-parsed value.\n",
					"\n",
					"    This method will less necessary after we complete our historical\n",
					"    event cleanup\n",
					"\n",
					"    Args:\n",
					"        json_str (string): Content of the `data` field in the events table\n",
					"\n",
					"    Returns:\n",
					"        Python dict, list, string, boolean, or number with json-parsed value.\n",
					"    \"\"\"\n",
					"    success = False\n",
					"    try:\n",
					"        return json.loads(json_str)\n",
					"    except ValueError:\n",
					"        pass\n",
					"    try:\n",
					"        return json.loads(json_str.replace(\"\\\\\\\\\", \"\\\\\"))\n",
					"    except ValueError:\n",
					"        pass\n",
					"    try:\n",
					"        return json.loads(json_str.replace(r'\\\\\"', \"\"))\n",
					"    except ValueError:\n",
					"        logging.warn(\"Unable to parse json string %s\", json_str)\n",
					"        return {}\n",
					"\n",
					"def _exchange_recipients(message, field):\n",
					"    recips = message.get(field, [])\n",
					"    if not isinstance(recips, list):\n",
					"        # TODO: is this really necessary?\n",
					"        recips = [recips]\n",
					"    recipients = []\n",
					"    for recip in recips:\n",
					"        if 'EmailAddress' in recip:\n",
					"            email = recip['EmailAddress']['Address']\n",
					"        else:\n",
					"            email = recip['emailAddress']['address']\n",
					"        recipients.append(EmailRecipient(email))\n",
					"    return recipients\n",
					"\n",
					"def _outlook_recipients(dataObj, field):\n",
					"    recips = dataObj.get(field, [])\n",
					"    recipients = []\n",
					"    if isinstance(recips, list):\n",
					"        recipients = recips\n",
					"    return recipients\n",
					"\n",
					"def _email_documents(docs, vsid_field, dvl_field, name_field):\n",
					"    return [EmailDocument(d.get(vsid_field), d.get(dvl_field))\n",
					"            for d in docs\n",
					"            if not d.get(name_field, \"\").endswith(\".ics\")\n",
					"            and d.get(vsid_field) is not None\n",
					"            and d.get(dvl_field) is not None]\n",
					"\n",
					"def _gmail_recipients(message, field):\n",
					"    recip_emails = message.get(field, [])\n",
					"    return [EmailRecipient(email) for email in recip_emails]\n",
					"\n",
					"def _log_warning(data_obj, action=None, include_data_obj_in_warning=False):\n",
					"    if action is None:\n",
					"        action = \"\"\n",
					"    else:\n",
					"        action = \" \" + action\n",
					"\n",
					"    if include_data_obj_in_warning:\n",
					"        logging.warning(\"Exception{0}: {1}\".format(action, data_obj), exc_info=True)\n",
					"    else:\n",
					"        logging.warning(\"Exception{0}\".format(action), exc_info=True)\n",
					"\n",
					"def _maybe_throw(action=None, include_data_obj_in_warning=False):\n",
					"    \"\"\"Decorate a function to maybe throw and maybe log a warning.\n",
					"\n",
					"    Any function decorated with _maybe_throw gains two new optional arguments:\n",
					"    throw (bool) and log_warning (bool)\n",
					"    \"\"\"\n",
					"    def maybe_throw(func):\n",
					"        @functools.wraps(func)\n",
					"        def wrapper(data_obj, throw=True, log_warning=True):\n",
					"            try:\n",
					"                return func(data_obj)\n",
					"            except:\n",
					"                if throw:\n",
					"                    raise\n",
					"                else:\n",
					"                    if log_warning:\n",
					"                        _log_warning(data_obj, action, include_data_obj_in_warning)\n",
					"                    return None\n",
					"        return wrapper\n",
					"    return maybe_throw\n",
					"\n",
					"def _repo_document(data_obj, vsid_fld, mod_time_fld):\n",
					"    \"\"\"Used to create a RepoDocument for various sources. The fld (field)\n",
					"    arguments are used to extract version_series_id and modified_at\n",
					"    values.\n",
					"\n",
					"    Args:\n",
					"        data_obj: (dict) parsed from event json\n",
					"        vsid_fld: (string) key name for version_series_id\n",
					"        mod_time_fld: (string) \"\" for modified_at\n",
					"    Returns:\n",
					"        RepoDocument\n",
					"    \"\"\"\n",
					"    return RepoDocument(data_obj[vsid_fld],\n",
					"            _parse_time(data_obj.get(mod_time_fld)))\n",
					"\n",
					"@_maybe_throw(\"parsing Exchange\")\n",
					"def parse_exchange(data_obj):\n",
					"    if 'Message' in data_obj:\n",
					"        message = data_obj['Message']\n",
					"        return ParsedEmail(\n",
					"            message['From']['EmailAddress']['Address'],\n",
					"            _exchange_recipients(message, 'ToRecipients'),\n",
					"            _exchange_recipients(message, 'CCRecipients'),\n",
					"            _email_documents(data_obj.get('Attachments', []),\n",
					"                         'VersionSeriesId', 'DocuratedVersionLabel', 'Name'),\n",
					"            data_obj.get(\"BitlyLinks\", []),\n",
					"            data_obj.get(\"BoxLinks\", []),\n",
					"            data_obj.get(\"GDriveLinks\", []),\n",
					"            data_obj.get(\"DocuratedLinks\", []),\n",
					"            data_obj.get(\"DropboxLinks\", []))\n",
					"    else:\n",
					"        message = data_obj['message']\n",
					"        return ParsedEmail(\n",
					"            message['from']['emailAddress']['address'],\n",
					"            _exchange_recipients(message, 'toRecipients'),\n",
					"            _exchange_recipients(message, 'ccRecipients'),\n",
					"            _email_documents(data_obj.get('attachments', []),'versionSeriesId', 'docuratedVersionLabel', 'name'),\n",
					"            data_obj.get(\"bitlyLinks\", []),\n",
					"            data_obj.get(\"boxLinks\", []),\n",
					"            data_obj.get(\"gdriveLinks\", []),\n",
					"            data_obj.get(\"docuratedLinks\", []),\n",
					"            data_obj.get(\"dropboxLinks\", []))\n",
					"\n",
					"@_maybe_throw(\"parsing Outlook\")\n",
					"def parse_outlook(data_obj):\n",
					"    return ParsedEmail(\n",
					"        data_obj.get(\"sender\", []),\n",
					"        _outlook_recipients(data_obj, \"to\"),\n",
					"        _outlook_recipients(data_obj, \"cc\"),\n",
					"        _email_documents(data_obj.get('attachments', []),'versionSeriesId', 'docuratedVersionLabel', 'name'),\n",
					"        data_obj.get(\"links\", []),\n",
					"        data_obj.get(\"boxLinks\", []),\n",
					"        data_obj.get(\"gdriveLinks\", []),\n",
					"        data_obj.get(\"docuratedLinks\", []),\n",
					"        data_obj.get(\"dropboxLinks\", []))\n",
					"\n",
					"@_maybe_throw(\"parsing Gmail\")\n",
					"def parse_gmail(data_obj):\n",
					"    return ParsedEmail(\n",
					"        data_obj[\"sender\"][0],\n",
					"        _gmail_recipients(data_obj, 'to'),\n",
					"        _gmail_recipients(data_obj, 'cc'),\n",
					"        _email_documents(data_obj.get('attachments', []),\n",
					"                         'version_series_id',\n",
					"                         'docurated_version_label',\n",
					"                         'name'),\n",
					"        data_obj.get(\"bitly_links\", []),\n",
					"        data_obj.get(\"box_links\", []),\n",
					"        data_obj.get(\"gdrive_links\", []),\n",
					"        data_obj.get(\"docurated_links\", []),\n",
					"        data_obj.get(\"dropbox_links\", []))\n",
					"\n",
					"@_maybe_throw(\"parsing presentation recipients\")\n",
					"def parse_presentation(data_obj):\n",
					"    \"\"\"Given a data_obj from a presentation event (e.g.\n",
					"    presentation_remote_updates_user-joins), return a\n",
					"    ParsedPresentation.\n",
					"\n",
					"    Args:\n",
					"        data_obj: dict parsed from event json\n",
					"\n",
					"    Returns:\n",
					"        ParsedPresentation\n",
					"    \"\"\"\n",
					"    recipient = data_obj['recipient']\n",
					"    if len(recipient) == 0:\n",
					"        recipients = []\n",
					"    else:\n",
					"        try:\n",
					"            recipients = json.loads(recipient[0])\n",
					"        except ValueError:\n",
					"            recipients = recipient\n",
					"    try:\n",
					"        iter(recipients)\n",
					"    except TypeError:\n",
					"        raise ValueError(\"Unable to parse recipients {0}\".format(recipients))\n",
					"    email = _parse_user_name(data_obj)\n",
					"    event = data_obj.get('event', data_obj['event_name'])\n",
					"    recipients = [EmailRecipient(r) for r in recipients if '@' in r]\n",
					"    docs = [SpaceDocument(o[\"id\"])\n",
					"            for o in data_obj['object'] if o['type'] == 'SPACE']\n",
					"    # In Presenter Events, we have asset_type=\"collection\", for collection related events.\n",
					"    collections = []\n",
					"    if (\"asset_type\" in data_obj) == True and (data_obj.get('asset_type') is not None):\n",
					"        asset_type = data_obj.get('asset_type')\n",
					"        if asset_type == \"collection\" and (\"asset_id\" in data_obj) == True and (data_obj.get('asset_id') is not None):\n",
					"            collections = data_obj.get('asset_id')\n",
					"            try:\n",
					"                collections = int(collections)\n",
					"            except:\n",
					"                collections = []\n",
					"                logging.warn(\"Unable to convert to Int %s\", collections)\n",
					"\n",
					"    return ParsedPresentation(data_obj['user_id'], email, event,\n",
					"            recipients, docs, collections)\n",
					"\n",
					"@_maybe_throw(\"parsing Box\")\n",
					"def parse_box(data_obj):\n",
					"    \"\"\"I dont think we even have any Box events in circulation right now,\n",
					"    so these fields might have to be changed in the future. Stay tuned.\n",
					"    \"\"\"\n",
					"    return ParsedRepoEvent(\n",
					"        data_obj[\"email\"],\n",
					"        _repo_document(data_obj, 'version_series_id', 'modified_date'))\n",
					"\n",
					"@_maybe_throw(\"parsing SharePoint\")\n",
					"def parse_share_point(data_obj):\n",
					"    \"\"\"SharePoint is missing a time field so we have to resort to using the\n",
					"    event time. SP also (possibly) has some missing vsids, so we need the\n",
					"    underlying_version_label to generate vsids if necessary.\n",
					"    \"\"\"\n",
					"    return ParsedRepoEvent(\n",
					"        data_obj.get('User') or data_obj[\"email\"],\n",
					"        _repo_document(data_obj, 'version_series_id', None))\n",
					"\n",
					"@_maybe_throw(\"parsing GoogleDrive\")\n",
					"def parse_google_drive(data_obj):\n",
					"    \"\"\"Historical GoogleDrive events are missing vsids, so we need the\n",
					"    underlying_id to generate vsids.\n",
					"    \"\"\"\n",
					"    vsid = data_obj.get('version_series_id')\n",
					"    if vsid is None:\n",
					"        underlying_id = data_obj.get('underlying_id')\n",
					"        if underlying_id is None:\n",
					"            return None\n",
					"        vsid = u\"GoogleDrive|{}\".format(underlying_id)\n",
					"    doc = RepoDocument(vsid, _parse_time(data_obj.get('modified_date')))\n",
					"    return ParsedRepoEvent(data_obj.get(\"email\") or data_obj['user'], doc)\n",
					"\n",
					"@_maybe_throw(\"parsing website\", True)\n",
					"def parse_website(data_obj):\n",
					"    user_id = data_obj['user_id']\n",
					"    email = _parse_user_name(data_obj)\n",
					"    docs = [SpaceDocument(o['id']) for o in data_obj['object']\n",
					"            if o['type'] == 'SPACE' and 'id' in o]\n",
					"    topics = [o['id'] for o in data_obj['object']\n",
					"            if o['type'] == 'TOPIC' and 'id' in o]\n",
					"    collections = [o['id'] for o in data_obj['object']\n",
					"            if o['type'] == 'COLLECTION' and 'id' in o]\n",
					"    return ParsedWebsiteEvent(user_id, email, docs, topics, collections)\n",
					"\n",
					"@_maybe_throw(\"parsing searches\", True)\n",
					"def parse_search_results(data_obj):\n",
					"    sr = json.loads(data_obj.get(\"search_results\", '[]'))\n",
					"    user_id = data_obj['user_id']\n",
					"    email = _parse_user_name(data_obj)\n",
					"    docs = [SpaceDocument(o['id']) for o in data_obj['object']\n",
					"            if o['type'] == 'SPACE' and 'id' in o]\n",
					"    topics = [o['id'] for o in data_obj['object']\n",
					"            if o['type'] == 'TOPIC' and 'id' in o]\n",
					"    collections = [o['id'] for o in data_obj['object']\n",
					"            if o['type'] == 'COLLECTION' and 'id' in o]\n",
					"    srdocs = [{\n",
					"                'id': srd['id'],\n",
					"                'klass': srd['klass'],\n",
					"                'space_id': srd['space_id']\n",
					"                }\n",
					"              for srd in sr if srd['klass'] == 'Space']\n",
					"    return ParsedSearchesRecord(user_id, email, docs, topics,\n",
					"                                collections, srdocs)\n",
					"\n",
					"def parse_event(source, data_obj, throw=True, log_warning=True):\n",
					"    args = [data_obj, throw, log_warning]\n",
					"    if source == 'website':\n",
					"        # this is to deal with presentation_create, which source = 'website'\n",
					"        if data_obj.get('controller') == 'search' or data_obj.get('controller') == 'plus/api/search':\n",
					"            return parse_search_results(*args)\n",
					"        if data_obj.get('controller') == 'presentation' or data_obj.get('controller') == 'plus/api/presentation':\n",
					"            return parse_presentation(*args)\n",
					"        else:\n",
					"            return parse_website(*args)\n",
					"    elif source == 'presenter':\n",
					"        return parse_presentation(*args)\n",
					"    elif source == 'GoogleDrive':\n",
					"        return parse_google_drive(*args)\n",
					"    elif source == 'SharePoint':\n",
					"        return parse_share_point(*args)\n",
					"    elif source == 'Gmail':\n",
					"        return parse_gmail(*args)\n",
					"    elif source == 'Box':\n",
					"        return parse_box(*args)\n",
					"    elif source == 'Exchange':\n",
					"        return parse_exchange(*args)\n",
					"    elif source == 'OutlookPlugin':\n",
					"        return parse_outlook(*args)\n",
					"    return None\n",
					"\n",
					"def space_clusters(spark, dump_dir, sDF):\n",
					"    # this is hoopty but is to deal with running tests and not being\n",
					"    # able to import graphframes\n",
					"    try:\n",
					"        from graphframes import GraphFrame\n",
					"    except:\n",
					"        print(\"Could not import graphframes\")\n",
					"        return None\n",
					"\n",
					"    spark.sparkContext.setCheckpointDir(dump_dir)\n",
					"    sDF = (sDF\n",
					"           .filter(\"source_file_type IS NOT NULL\")\n",
					"           .filter(\"source_file_type NOT IN ('Image')\"))\n",
					"\n",
					"    # if ENV_VARS.get('ENV', 'development') == 'production':\n",
					"    #     sDF = sDF.filter(\"organization_id != 677\")\n",
					"    #     sDF = sDF.filter(\"upload_source != 'Desktop'\")\n",
					"\n",
					"    emtd_org_ids = (1, 1112, 1123, 1109, 8, 1118, 1117, 1126, 1120, 1125, 863, 1110, 1122, 7, 1127, 1143, 1145, 1144, 1149, 1121, 1148, 1146, 1153, 1154, 1160, 1157, 1158, 1150, 1159, 1161, 1152, 1162, 1170, 1166, 1165, 1168, 1167, 1174, 1169, 1172, 1164, 1173, 1178, 1171, 891, 1155, 1119, 1124, 1179, 1180, 1183, 1187, 1182, 1181, 1189, 1185, 1188, 1186)\n",
					"    if emtd_org_ids:\n",
					"        sDF = sDF.filter(\"organization_id NOT IN {0}\".format(emtd_org_ids))\n",
					"    vertices = sDF.select('id', 'organization_id', 'content_identifier',\n",
					"                          'version_series_id', 'visible')\n",
					"\n",
					"    sDFci = sDF.select('id', 'organization_id', 'content_identifier')\n",
					"    cid_edges = (\n",
					"        sDFci\n",
					"        .alias('src')\n",
					"        .join(sDFci.alias('dst'), ['organization_id', 'content_identifier'])\n",
					"        .select(\n",
					"            col('src.organization_id'),\n",
					"            col('src.id').alias('src'),\n",
					"            col('dst.id').alias('dst'))\n",
					"        .filter(\"src != dst\"))\n",
					"\n",
					"    sDFvsid = sDF.select('id', 'organization_id', 'version_series_id')\n",
					"    vsid_edges = (\n",
					"        sDFvsid.alias('src')\n",
					"        .join(sDFvsid.alias('dst'), ['organization_id', 'version_series_id'])\n",
					"        .select(\n",
					"            col('src.organization_id'),\n",
					"            col('src.id').alias('src'),\n",
					"            col('dst.id').alias('dst'))\n",
					"        .filter(\"src != dst\"))\n",
					"    edges = cid_edges.filter((cid_edges['organization_id'] != 1112) & (cid_edges['organization_id'] != 3)).union(vsid_edges)\n",
					"\n",
					"    opts = {}\n",
					"    opts['ext'] = 'csv'\n",
					"\n",
					"    res = GraphFrame(vertices, edges).connectedComponents()\n",
					"    res.cache()\n",
					"\n",
					"    visibleDF = res.filter('visible == True')\n",
					"    inVisibleDF = res.filter('visible == False')\n",
					"\n",
					"    visibleMaxes = (\n",
					"        visibleDF\n",
					"        .groupBy('component').max('id')\n",
					"        .select('component', col('max(id)').alias('cr_space_id')))\n",
					"\n",
					"    inVisibleMaxes = (\n",
					"        inVisibleDF\n",
					"        .groupBy('component').max('id')\n",
					"        .select('component', col('max(id)').alias('cr_space_id')))\n",
					"\n",
					"    finalDF = (\n",
					"        visibleMaxes.alias('trueVisible')\n",
					"        .join(inVisibleMaxes.alias('falseVisible'), 'component', 'full')\n",
					"        .select(\n",
					"            col('component'),\n",
					"            col('trueVisible.cr_space_id').alias('true_space_id'),\n",
					"            col('falseVisible.cr_space_id').alias('false_space_id')))\n",
					"    finalDF = (\n",
					"        finalDF.withColumn(\"true_space_id\", coalesce(finalDF.true_space_id, finalDF.false_space_id))\n",
					"        .select('component', col('true_space_id').alias('cr_space_id')))\n",
					"\n",
					"    return (res\n",
					"            .join(finalDF, 'component')\n",
					"            .select(\n",
					"                'organization_id',\n",
					"                col('id').alias('space_id'),\n",
					"                'content_identifier',\n",
					"                'version_series_id',\n",
					"                'cr_space_id')\n",
					"            .withColumn('cluster_id', col('cr_space_id')))\n",
					"\n",
					"def generate_cluster_trends(spark, user_event_opps, event_docs,\n",
					"                            today=date.today(), num_weeks=4):\n",
					"    date_3w = today - timedelta(weeks=num_weeks)\n",
					"    date_1w = today - timedelta(weeks=1)\n",
					"    ue = (user_event_opps\n",
					"          .select(\n",
					"              'event_skey',\n",
					"              col('time').cast('date').alias('date')))\n",
					"    ed = event_docs.select('event_skey', 'cluster_id')\n",
					"    avg_3w = (ue\n",
					"              .filter(expr(\"date >= '%s' AND date <= '%s'\" % (date_3w, date_1w)))\n",
					"              .join(ed, 'event_skey')\n",
					"              .groupBy('cluster_id', 'date')\n",
					"              .count()\n",
					"              .groupBy('cluster_id', window('date', \"1 week\"))\n",
					"              .agg(avg('count').alias('weekly_avg'))\n",
					"              .groupBy('cluster_id')\n",
					"              .agg(avg('weekly_avg').alias('avg_3w')))\n",
					"    avg_1w = (ue\n",
					"              .filter(expr(\"date >= '%s'\" % date_1w))\n",
					"              .join(ed, 'event_skey')\n",
					"              .groupBy('cluster_id')\n",
					"              .count())\n",
					"    return (avg_3w\n",
					"            .join(avg_1w, 'cluster_id', 'outer')\n",
					"            .na.fill({'count': 0, 'avg_3w': 0})\n",
					"            .withColumn('trend', expr('(count - avg_3w) / avg_3w'))\n",
					"            .drop('count', 'avg_3w'))\n",
					"\n",
					"def cluster_dim(spark, dump_dir, spaceDim):\n",
					"    sc = space_clusters(spark, dump_dir, spaceDim)\n",
					"    if sc is None:\n",
					"        return None\n",
					"    return sc.withColumn(\"skey\", monotonically_increasing_id())\n",
					"\n",
					"OPPORTUNITY_EVENT_NAMES = get_events_for_type(OPPORTUNITY)\n",
					"COMPONENT_EVENT_NAMES = get_events_for_type([COMPONENT_START, COMPONENT_END, COMPONENT_START_INTERNAL, COMPONENT_END_INTERNAL])\n",
					"\n",
					"base_flds = [\n",
					"        StructField(\"skey\", LongType(), False),\n",
					"        StructField(\"organization_id\", IntegerType(), False),\n",
					"        StructField(\"guid\", StringType(), False),\n",
					"        StructField(\"source\", StringType(), False),\n",
					"        StructField(\"event_name\", StringType(), False),\n",
					"        StructField(\"time\", StringType(), False),\n",
					"        StructField(\"user_id\", IntegerType(), True)\n",
					"        ]\n",
					"\n",
					"one_flds = [\n",
					"        StructField(\"email\", StringType(), True),\n",
					"        StructField(\"search_session_id\", StringType(), True),\n",
					"        StructField(\"query\", StringType(), True),\n",
					"        StructField(\"total_results\", IntegerType(), True),\n",
					"        StructField(\"os\", StringType(), True),\n",
					"        StructField(\"opportunity_id\", StringType(), True),\n",
					"        StructField(\"caller\", StringType(), True),\n",
					"        StructField(\"component_id\", IntegerType(), True),\n",
					"        StructField(\"link_id\", IntegerType(), True)\n",
					"        ]\n",
					"\n",
					"many_flds = [\n",
					"        StructField(\"documents\", ArrayType(\n",
					"            StructType([\n",
					"                StructField(\"space_id\", IntegerType(), True),\n",
					"                StructField(\"version_series_id\", StringType(), True),\n",
					"                StructField(\"docurated_version_label\", LongType(), True),\n",
					"                StructField(\"modified_at\", StringType(), True)\n",
					"            ]), False\n",
					"        ), True),\n",
					"        StructField(\"recipients\", ArrayType(\n",
					"            StructType([\n",
					"                StructField(\"recipient_email\", StringType(), False),\n",
					"                StructField(\"recipient_domain\", StringType(), False)\n",
					"            ]), False\n",
					"        ), True),\n",
					"        StructField(\"topics\", ArrayType(\n",
					"            StructType([\n",
					"                StructField(\"topic_id\", IntegerType(), False)\n",
					"            ]), False\n",
					"        ), True),\n",
					"        StructField(\"collections\", ArrayType(\n",
					"            StructType([\n",
					"                StructField(\"collection_id\", IntegerType(), False)\n",
					"            ]), False\n",
					"        ), True),\n",
					"        StructField(\"search_results\", ArrayType(\n",
					"            StructType([\n",
					"                StructField(\"id\", IntegerType(), True),\n",
					"                StructField(\"klass\", StringType(), True),\n",
					"                StructField(\"space_id\", IntegerType(), True),\n",
					"                ]), False\n",
					"        ), True)\n",
					"        ]\n",
					"event_schema = StructType(base_flds + one_flds + many_flds)\n",
					"base_cols = [f.name for f in base_flds]\n",
					"one_val_cols = [f.name for f in one_flds]\n",
					"many_val_cols = [f.name for f in many_flds]\n",
					"\n",
					"def parse_data(json_str):\n",
					"    \"\"\"\n",
					"    Given the string content of the `data` field in the events table,\n",
					"    returns a Python type with the json-parsed value.\n",
					"\n",
					"    This method will less necessary after we complete our historical\n",
					"    event cleanup\n",
					"\n",
					"    Args:\n",
					"        json_str (string): Content of the `data` field in the events table\n",
					"\n",
					"    Returns:\n",
					"        Python dict, list, string, boolean, or number with json-parsed value.\n",
					"    \"\"\"\n",
					"    success = False\n",
					"    try:\n",
					"        return json.loads(json_str)\n",
					"    except ValueError:\n",
					"        pass\n",
					"    try:\n",
					"        return json.loads(json_str.replace(\"\\\\\\\\\", \"\\\\\"))\n",
					"    except ValueError:\n",
					"        pass\n",
					"    try:\n",
					"        return json.loads(json_str.replace(r'\\\\\"', \"\"))\n",
					"    except ValueError:\n",
					"        logging.warn(\"Unable to parse json string %s\", json_str)\n",
					"        return {}\n",
					"\n",
					"def parse_event(source, data_obj, throw=True, log_warning=True):\n",
					"    args = [data_obj, throw, log_warning]\n",
					"    if source == 'website':\n",
					"        # this is to deal with presentation_create, which source = 'website'\n",
					"        if data_obj.get('controller') == 'search' or data_obj.get('controller') == 'plus/api/search':\n",
					"            return parse_search_results(*args)\n",
					"        if data_obj.get('controller') == 'presentation' or data_obj.get('controller') == 'plus/api/presentation':\n",
					"            return parse_presentation(*args)\n",
					"        else:\n",
					"            return parse_website(*args)\n",
					"    elif source == 'presenter':\n",
					"        return parse_presentation(*args)\n",
					"    elif source == 'GoogleDrive':\n",
					"        return parse_google_drive(*args)\n",
					"    elif source == 'SharePoint':\n",
					"        return parse_share_point(*args)\n",
					"    elif source == 'Gmail':\n",
					"        return parse_gmail(*args)\n",
					"    elif source == 'Box':\n",
					"        return parse_box(*args)\n",
					"    elif source == 'Exchange':\n",
					"        return parse_exchange(*args)\n",
					"    elif source == 'OutlookPlugin':\n",
					"        return parse_outlook(*args)\n",
					"    return None\n",
					"\n",
					"def parse_search_session_id(data):\n",
					"    s = data.get('search_session_id')\n",
					"    if data.get('ajax', '') == 'true':\n",
					"        return None\n",
					"    if s is None or s == '' or s == 'null':\n",
					"        return None\n",
					"    return s\n",
					"\n",
					"def get_int(data, name):\n",
					"    try:\n",
					"        return int(data.get(name))\n",
					"    except:\n",
					"        return None\n",
					"\n",
					"def parse_os(data):\n",
					"    s = data.get('os')\n",
					"    if s is None or s == '' or s == 'null':\n",
					"        return None\n",
					"    return s\n",
					"\n",
					"def parse_caller(data):\n",
					"    output = 'Website'\n",
					"    if 'chrome' in data.get('caller', 'web'):\n",
					"        output = 'Chrome Extension'\n",
					"    elif 'gmail' in data.get('caller', 'web'):\n",
					"        output = 'Gmail'\n",
					"    elif 'outlook' in data.get('caller', 'web'):\n",
					"        output = 'Outlook'\n",
					"    elif 'sales' in data.get('caller', 'web'):\n",
					"        output = 'Salesforce'\n",
					"    elif 'app' in data.get('caller', 'web'):\n",
					"        output = 'iOS App'\n",
					"    return output\n",
					"\n",
					"def parse_opportunity(data):\n",
					"    opps = data.get('opportunities', [])\n",
					"    try:\n",
					"        if len(opps) > 0:\n",
					"            return opps[0]\n",
					"        opps = data.get('objects', [])\n",
					"        if len(opps) > 0:\n",
					"            return opps[0]\n",
					"        return None\n",
					"    except:\n",
					"        return None\n",
					"\n",
					"def unroll_search_results(pe):\n",
					"    output = []\n",
					"    if type(pe) == ParsedSearchesRecord:\n",
					"        for d in pe.search_records:\n",
					"            output.append(d)\n",
					"    return output\n",
					"\n",
					"def unroll_documents(pe, e):\n",
					"    output = []\n",
					"    for d in pe.documents:\n",
					"        obj = {}\n",
					"        if type(d) == SpaceDocument:\n",
					"            obj['space_id'] = d.space_id\n",
					"        elif type(d) == RepoDocument:\n",
					"            obj['version_series_id'] = d.version_series_id\n",
					"            obj['modified_at'] = d.modified_at or e.time\n",
					"        elif type(d) == EmailDocument:\n",
					"            obj['version_series_id'] = d.version_series_id\n",
					"            obj['docurated_version_label'] = d.docurated_version_label\n",
					"        else:\n",
					"            logging.warn(\"ERROR: Found Invalid Document object %s\", d)\n",
					"        output.append(obj)\n",
					"    return output\n",
					"\n",
					"def unroll_recipients(pe):\n",
					"    output = []\n",
					"    if type(pe) == ParsedPresentation or type(pe) == ParsedEmail:\n",
					"        for r in pe.recipients:\n",
					"            if not isinstance(r, str):\n",
					"                if r.email is not None and r.domain is not None:\n",
					"                    output.append({'recipient_email': r.email, 'recipient_domain': r.domain})\n",
					"    return output\n",
					"\n",
					"def unroll_topics(pe):\n",
					"    output = []\n",
					"    if type(pe) == ParsedWebsiteEvent and len(pe.topics) > 0:\n",
					"        for t_id in pe.topics:\n",
					"            output.append({'topic_id': t_id})\n",
					"    return output\n",
					"\n",
					"def unroll_collections(pe):\n",
					"    output = []\n",
					"    if (type(pe) == ParsedWebsiteEvent or type(pe) == ParsedPresentation) and len(pe.collections) > 0:\n",
					"        for c_id in pe.collections:\n",
					"            output.append({'collection_id': c_id})\n",
					"    return output\n",
					"\n",
					"def parse_component(data):\n",
					"    c_id = data.get('component_id')\n",
					"    if c_id is None or c_id == '' or c_id == 'null':\n",
					"        return None\n",
					"    return c_id\n",
					"\n",
					"def process_event(e):\n",
					"    data = parse_data(e.data)\n",
					"    if data is {}:\n",
					"        return None\n",
					"    pe = parse_event(e.source, data, False, False)\n",
					"    if pe is None:\n",
					"        return None\n",
					"    obj = e.asDict()\n",
					"    obj.pop('data')\n",
					"\n",
					"    # here is where we add 1-to-1 columns\n",
					"    if obj['email'] is None:\n",
					"        if type(pe) == ParsedEmail:\n",
					"            obj['email'] = pe.sender\n",
					"        else:\n",
					"            obj['email'] = pe.email\n",
					"    obj['search_session_id'] = parse_search_session_id(data)\n",
					"    obj['query'] = data.get('query')\n",
					"    obj['total_results'] = get_int(data, 'total_results')\n",
					"    obj['os'] = parse_os(data)\n",
					"    obj['caller'] = parse_caller(data)\n",
					"\n",
					"    if e.event_name in OPPORTUNITY_EVENT_NAMES:\n",
					"        obj['opportunity_id'] = parse_opportunity(data)\n",
					"\n",
					"    # here is where we add many-to-many columns\n",
					"    obj['search_results'] = unroll_search_results(pe)\n",
					"    obj['documents'] = unroll_documents(pe, e)\n",
					"    obj['recipients'] = unroll_recipients(pe)\n",
					"    obj['topics'] = unroll_topics(pe)\n",
					"    obj['collections'] = unroll_collections(pe)\n",
					"\n",
					"    if e.event_name in COMPONENT_EVENT_NAMES:\n",
					"        obj['component_id'] = parse_component(data)\n",
					"        obj['link_id'] = data.get('link_id')\n",
					"\n",
					"    return obj\n",
					"\n",
					"def event_filter():\n",
					"    conds = []\n",
					"    for source, vals in EVENT_TYPE_MAP.items():\n",
					"        for e_name, e_type in vals.items():\n",
					"            c = \"(source = '%s' AND event_name = '%s')\" % (source, e_name)\n",
					"            conds.append(c)\n",
					"    return ' OR '.join(conds)\n",
					"\n",
					"def unroll_events(spark, events, users):\n",
					"    u = users.select('email', users.id.alias('user_id'))\n",
					"    event_rdd = (events\n",
					"                 .filter(event_filter())\n",
					"                 .filter(\"organization_id > 0\")\n",
					"                 .join(broadcast(u), 'user_id', 'left')  # add missing emails\n",
					"                 .select(*base_cols, 'data', 'email')\n",
					"                 .rdd\n",
					"                 .map(process_event).filter(lambda l: l is not None))\n",
					"    return spark.createDataFrame(event_rdd, event_schema)\n",
					"\n",
					"\n",
					"def process_events(spark, events, users):\n",
					"    return (unroll_events(spark, events, users)\n",
					"            .select(\n",
					"                *base_cols,\n",
					"                *one_val_cols,\n",
					"                *many_val_cols))\n",
					"\n",
					"def process_eventss(unrolled_events):\n",
					"    return (unrolled_events\n",
					"        .withColumn('rec', explode('recipients'))\n",
					"        .select(\n",
					"            col('skey').alias('event_skey'),\n",
					"            'organization_id',\n",
					"            col('rec.recipient_email').alias('email'),\n",
					"            col('rec.recipient_domain').alias('domain'))\n",
					"        .distinct())\n",
					"\n",
					"users = user_dim(spark, '/')\n",
					"events = event_dim(spark, '/')\n",
					"unrolled_events = process_events(spark, events, users)\n",
					"\n",
					"def domains_and_recipients(unrolled_events):\n",
					"    recipDFtemp = process_eventss(unrolled_events)\n",
					"    recipDFtemp.cache()\n",
					"\n",
					"    recipients = (recipDFtemp\n",
					"        .select('email', 'organization_id')\n",
					"        .distinct()\n",
					"        .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"    domains = (recipDFtemp\n",
					"        .select('domain', 'organization_id')\n",
					"        .distinct()\n",
					"        .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"\n",
					"    event_recipients = (recipDFtemp\n",
					"        .join(recipients.alias(\"recipients\"), 'email')\n",
					"        .select(recipients.skey.alias('recipient_skey'), 'event_skey', 'email',\n",
					"            col('recipients.organization_id'))\n",
					"        .distinct())\n",
					"    event_domains = (recipDFtemp\n",
					"        .join(domains.alias(\"domains\"), 'domain')\n",
					"        .select(domains.skey.alias('domain_skey'), 'event_skey', 'domain',\n",
					"            col('domains.organization_id'))\n",
					"        .distinct())\n",
					"    return dict(domains=domains, recipients=recipients,\n",
					"            event_domains=event_domains, event_recipients=event_recipients)\n",
					"\n",
					"def search_dim(spark, events):\n",
					"    search_names = get_events_for_type(SEARCH, WEBSITE)\n",
					"    search_str = \" OR \".join([\"event_name = '%s'\" % e for e in search_names])\n",
					"    queries = (events\n",
					"               .filter(search_str)\n",
					"               .filter('search_session_id IS NOT NULL AND query IS NOT NULL')\n",
					"               .select(\n",
					"                   'organization_id',\n",
					"                   'search_session_id',\n",
					"                   'query',\n",
					"                   col('total_results').alias('result_count'))\n",
					"               .groupBy('organization_id', 'search_session_id', 'query')\n",
					"               .agg(max('result_count').alias('result_count'))\n",
					"               .withColumn('search_event', lit(1))\n",
					"               .withColumn('query_lowercase', lower(col('query'))))\n",
					"    view_names = (get_events_for_type(VIEW, WEBSITE) +\n",
					"                  get_events_for_type(OPEN, WEBSITE))\n",
					"    view_str = \" OR \".join([\"event_name = '%s'\" % e for e in view_names])\n",
					"    views = (events\n",
					"             .filter(view_str)\n",
					"             .filter('search_session_id IS NOT NULL')\n",
					"             .groupBy('organization_id', 'search_session_id')\n",
					"             .count()\n",
					"             .select(\n",
					"                 'organization_id',\n",
					"                 'search_session_id',\n",
					"                 col('count').alias('view_count')))\n",
					"    df = (queries\n",
					"          .join(views, ['organization_id', 'search_session_id'], 'left')\n",
					"          .withColumn('search_event',\n",
					"                      expr('search_event IS NOT NULL').cast('integer'))\n",
					"          .withColumn('no_results',\n",
					"                      expr('result_count = 0').cast('integer'))\n",
					"          .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"    null_opts = {'skey': -1, 'search_event': 0, 'no_results': 0,\n",
					"                 'organization_id': -1}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    return df.union(null_row)\n",
					"\n",
					"def os_dim(spark, unrolled_events):\n",
					"    df = (unrolled_events\n",
					"          .select(col('os').alias('name'))\n",
					"          .distinct()\n",
					"          .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"    null_opts = {'skey': -1, 'name': 'OS n/a'}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    return df.union(null_row)\n",
					"\n",
					"def caller_dim(spark, unrolled_events):\n",
					"    df = (unrolled_events\n",
					"          .select(col('caller').alias('name'))\n",
					"          .distinct()\n",
					"          .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"    null_opts = {'skey': -1, 'name': 'CALLER n/a'}\n",
					"    null_row = spark.createDataFrame([null_opts], schema=df.schema)\n",
					"    return df.union(null_row)\n",
					"\n",
					"def non_derived_dimensions(spark, work_dir):\n",
					"    sp, wd = spark, work_dir\n",
					"    dims = {}\n",
					"\n",
					"    dims['organizations'] = organization_dim(sp, wd)\n",
					"    dims['users'] = user_dim(sp, wd)\n",
					"    dims['groups'] = group_dim(sp, wd)\n",
					"    dims['users'].cache()\n",
					"    dims['groups'].cache()\n",
					"    dims['group_users'], dims['users'] = group_users_dim(sp, wd,\n",
					"                                                         dims['groups'],\n",
					"                                                         dims['users'])\n",
					"\n",
					"    dims['event_names'] = event_name_dim(sp)\n",
					"    dims['sources'] = source_dim(sp)\n",
					"\n",
					"    dims['spaces'] = space_dim(sp, wd)\n",
					"    dims['topics'] = topic_dim(sp, wd)\n",
					"    dims['collections'] = collection_dim(sp, wd)\n",
					"    dims['components'] = components_dim(sp, wd)\n",
					"    dims['spaces'].cache()\n",
					"    dims['topics'].cache()\n",
					"    dims['collections'].cache()\n",
					"    dims['spaces_topics'], dims['spaces'] = spaces_topics_dim(sp, wd,\n",
					"                                                              dims['spaces'],\n",
					"                                                              dims['topics'])\n",
					"    dims['collections_items'] = collections_items_dim(sp, wd, dims['spaces'], dims['collections'])\n",
					"    dims['space_extras'] = space_extras_dim(sp, wd, dims['spaces'])\n",
					"    dims['clusters'] = cluster_dim(sp, wd, dims['spaces'])\n",
					"    dims['items_components'] = items_components_dim(sp, wd, dims['spaces'], dims['components'])\n",
					"\n",
					"    dims['sf_opportunities'] = sf_opportunity_dim(sp, wd)\n",
					"    dims['sf_accounts'] = sf_account_dim(sp, wd)\n",
					"    dims['sf_contacts'] = sf_contact_dim(sp, wd)\n",
					"    dims['sf_opp_histories'] = sf_opp_history_dim(sp, wd)\n",
					"    dims['sf_users'] = sf_user_dim(sp, wd)\n",
					"    dims['sf_oppcontroles'] = sf_oppcontrols_dim(sp, wd)\n",
					"    dims['sf_reps'] = sf_rep_dim(dims)\n",
					"\n",
					"    dims['tableau_users'] = tableau_user_dim(sp, wd)\n",
					"    dims['content_scores'] = content_score_dim(sp, wd)\n",
					"\n",
					"    sync_meta_orgs = [1156]\n",
					"    if type(sync_meta_orgs) is str:\n",
					"      sync_meta_orgs = eval(sync_meta_orgs)\n",
					"\n",
					"    for sync_org in sync_meta_orgs:\n",
					"        logging.info(\"INFO: SPARK SYNC_METADATA ORG {0}\".format(sync_org))\n",
					"        dim_name = 'sync_metadata_' + str(sync_org)\n",
					"        dims[dim_name] = sync_metadata_dim(sp, wd, dims['spaces'], sync_org)\n",
					"\n",
					"        if \"source_asset_name\" in dims[dim_name].columns:\n",
					"            if dims.get('cep_sources_dim', None) is None:\n",
					"                dims['cep_sources_dim'] = dims[dim_name]\n",
					"            else:\n",
					"                dims['cep_sources_dim'] = dims['cep_sources_dim'].union(dims[dim_name])\n",
					"    if dims.get('cep_sources_dim', None) is not None:\n",
					"        dims['cep_sources_dim'] = process_cep_sources_dim(dims['cep_sources_dim'], dims['spaces'])\n",
					"    return dims\n",
					"\n",
					"def derived_dimensions(spark, unrolled_events):\n",
					"    ddims = domains_and_recipients(unrolled_events)\n",
					"    ddims['searches'] = search_dim(spark, unrolled_events)\n",
					"    ddims['oses'] = os_dim(spark, unrolled_events)\n",
					"    ddims['callers'] = caller_dim(spark, unrolled_events)\n",
					"    return ddims\n",
					"\n",
					"wd = '/'\n",
					"dims = non_derived_dimensions(spark, wd)\n",
					"dims.update(derived_dimensions(spark, unrolled_events))\n",
					"\n",
					"DOMAIN_BLACKLIST = ['gmail.com', 'hotmail.com', 'yahoo.com', 'outlook.com',\n",
					"                    'aol.com', 'att.com']\n",
					"ORG_DOMAIN_BLACKLIST = {\n",
					"    1: ['docurated.com'],\n",
					"    23: ['docurated.com'],\n",
					"    481: ['outbrain.com'],\n",
					"    583: ['usbioservices.com', 'intrinsiq.com', 'worldcourier.com',\n",
					"          'iononline.com', 'premiersource.com', 'innomarstrategies.com',\n",
					"          'oncologysupply.com', 'besse.com', 'lashgroup.com', 'icsconnect.com',\n",
					"          'asdhealthcare.com', 'xcenda.com', 'absg.com',\n",
					"          'amerisourcebergen.com'],\n",
					"    602: ['dynamicyield.com'],\n",
					"    617: ['condenast.com'],\n",
					"    567: ['riverbed.com'],\n",
					"    568: ['couponsinc.com', 'quotient.com'],\n",
					"    616: ['appboy.com'],\n",
					"    442: ['appirio.com', 'topcoder.com', 'appirio-dev3.com',\n",
					"          'appirio-dev2.com', 'appirio-dev1.com'],\n",
					"    551: ['valassis.com'],\n",
					"    588: [\"infoblox.com\", \"infoblox.onmicrosoft.com\"],\n",
					"    445: ['acquia.com'],\n",
					"    458: [\"docurated.rocks\", \"o365.docurated.com\"],\n",
					"    621: [\"dieboldnixdorf.com\"],\n",
					"    627: [\"quark.com\"],\n",
					"    628: [\"epicor.com\"]\n",
					"}\n",
					"\n",
					"\n",
					"def get_multi_domains_bl(contacts):\n",
					"    return (contacts\n",
					"            .withColumn('domain', lower(split('email', '@').getItem(1)))\n",
					"            .select('organization_id', 'domain', 'account_id')\n",
					"            .distinct()\n",
					"            .groupBy('organization_id', 'domain')\n",
					"            .count()\n",
					"            .filter('count > 15')\n",
					"            .select('organization_id', 'domain')\n",
					"            .withColumn('blacklisted', lit(1)))\n",
					"\n",
					"\n",
					"def org_domain_bl(spark):\n",
					"    arr = []\n",
					"    for org_id, blist in ORG_DOMAIN_BLACKLIST.items():\n",
					"        for d in blist:\n",
					"            arr.append({\n",
					"                'organization_id': org_id, 'domain': d, 'blacklisted': 1})\n",
					"    bl_schema = StructType([\n",
					"        StructField(\"organization_id\", IntegerType(), False),\n",
					"        StructField(\"domain\", StringType(), False),\n",
					"        StructField(\"blacklisted\", IntegerType(), False)])\n",
					"    return spark.createDataFrame(arr, bl_schema)\n",
					"\n",
					"\n",
					"def join_dimensions(df, dims):\n",
					"    orgDim = (dims['organizations']\n",
					"              .select(\n",
					"                  col('skey').alias('organization_skey'),\n",
					"                  col('id').alias('organization_id')))\n",
					"    userDim = (dims['users']\n",
					"               .select(\n",
					"                   col('skey').alias('user_skey'), 'organization_id', 'email'))\n",
					"    enameDim = (dims['event_names']\n",
					"                .select(\n",
					"                    col('skey').alias('event_name_skey'),\n",
					"                    col('name').alias('event_name'),\n",
					"                    col('type')))\n",
					"    sourceDim = (dims['sources']\n",
					"                 .select(\n",
					"                     col('skey').alias('source_skey'),\n",
					"                     col('name').alias('source')))\n",
					"    osDim = (dims['oses']\n",
					"             .select(\n",
					"                 col('skey').alias('os_skey'),\n",
					"                 col('name').alias('os')))\n",
					"    calerDim = (dims['callers']\n",
					"             .select(\n",
					"                 col('skey').alias('caller_skey'),\n",
					"                 col('name').alias('caller')))\n",
					"    return (df\n",
					"            .join(orgDim, 'organization_id')\n",
					"            .join(userDim, ['organization_id', 'email'])\n",
					"            .join(enameDim, 'event_name')\n",
					"            .join(sourceDim, 'source')\n",
					"            .join(osDim, 'os', 'left')\n",
					"            .join(calerDim, 'caller', 'left')\n",
					"            .na.fill({'os_skey': -1}))\n",
					"\n",
					"def generate_user_events(unrolled_events, dims):\n",
					"    eventsDF = (unrolled_events\n",
					"                .select(\n",
					"                    col('skey').alias('event_skey'),\n",
					"                    'email',\n",
					"                    'organization_id',\n",
					"                    'guid',\n",
					"                    'source',\n",
					"                    'event_name',\n",
					"                    'time',\n",
					"                    'caller',\n",
					"                    'os')\n",
					"                .withColumn('date_skey',\n",
					"                            date_format('time', 'yyyyMMdd').cast('int'))\n",
					"                .distinct())\n",
					"    return (join_dimensions(eventsDF, dims)\n",
					"            .select(\n",
					"                'event_skey',\n",
					"                'organization_skey',\n",
					"                'user_skey',\n",
					"                'source_skey',\n",
					"                'event_name_skey',\n",
					"                'type',\n",
					"                'date_skey',\n",
					"                'os_skey',\n",
					"                'caller_skey',\n",
					"                'organization_id',\n",
					"                'guid',\n",
					"                unix_timestamp('time',\n",
					"                               TIME_FMT).cast('timestamp').alias('time')))\n",
					"\n",
					"def convert_events(unrolled_events):\n",
					"    return (unrolled_events\n",
					"        .withColumn('doc', explode('documents'))\n",
					"        .select(\n",
					"            col('skey').alias('event_skey'),\n",
					"            'organization_id',\n",
					"            'source',\n",
					"            'doc.*')\n",
					"        .distinct())\n",
					"\n",
					"def join_on_sid(spark, deDF, sDF):\n",
					"    join_cond = [\n",
					"        deDF.organization_id == sDF.organization_id,\n",
					"        deDF.space_id == sDF.id ]\n",
					"    # sDF = sDF.select('organization_id', sDF.id.alias('space_id'))\n",
					"    return (deDF\n",
					"        .select('event_skey', 'organization_id', 'space_id')\n",
					"        .join(sDF, join_cond)\n",
					"        .select('event_skey', sDF.id.alias('space_id')))\n",
					"\n",
					"def join_on_vsid_doc_v_label(spark, deDF, sDF):\n",
					"    cols = ['organization_id', 'version_series_id', 'docurated_version_label']\n",
					"    return (deDF\n",
					"        .join(sDF, cols)\n",
					"        .select('event_skey', sDF.id.alias('space_id')))\n",
					"\n",
					"def doc_ev_key(de):\n",
					"    mod_times = (de['space_id'], de['modified_at'], de['doc_ev_modified_at'])\n",
					"    return (de['event_skey'], mod_times)\n",
					"\n",
					"def reduce_doc_ev_spaces(des1, des2):\n",
					"    #des tuple = (space_id, modified_at, doc_ev_modified_at)\n",
					"    space_id = 0\n",
					"    modified_at = 1\n",
					"    doc_ev_modified_at = 2\n",
					"\n",
					"    mod_at_sid_sort = lambda d: (d[modified_at], d[space_id])\n",
					"    earlier_des, later_des = sorted([des1,des2], key=mod_at_sid_sort)\n",
					"\n",
					"    if des1[modified_at] == des2[modified_at]:\n",
					"        return earlier_des\n",
					"    elif des1[doc_ev_modified_at] < later_des[modified_at]:\n",
					"        return earlier_des\n",
					"    else:\n",
					"        return later_des\n",
					"\n",
					"def parse_reduce_pairs(pair):\n",
					"    key, mod_times = pair\n",
					"    space_id, _, _ = mod_times\n",
					"    return { 'event_skey': key, 'space_id': space_id }\n",
					"\n",
					"doc_ev_space_cols = [\n",
					"  (\"event_skey\", LongType(), False),\n",
					"  (\"space_id\", IntegerType(), False)\n",
					"]\n",
					"doc_ev_space_schema = StructType([StructField(*c) for c in doc_ev_space_cols])\n",
					"\n",
					"def join_on_vsid_mod_date(spark, deDF, sDF):\n",
					"    cols = [\n",
					"        'event_skey',\n",
					"        sDF.id.alias('space_id'),\n",
					"        sDF.modified_at,\n",
					"        deDF.modified_at.alias('doc_ev_modified_at') ]\n",
					"    doc_ev_by_vsid_rdd = (deDF\n",
					"        .join(sDF, ['organization_id', 'version_series_id'])\n",
					"        .select(*cols)\n",
					"        .rdd\n",
					"        .map(doc_ev_key)\n",
					"        .reduceByKey(reduce_doc_ev_spaces)\n",
					"        .map(parse_reduce_pairs))\n",
					"    return spark.createDataFrame(doc_ev_by_vsid_rdd, doc_ev_space_schema)\n",
					"\n",
					"def join_on_vsid(spark, deDF, sDF):\n",
					"    deDF.cache()\n",
					"    de_doc_v_labelDF = deDF.filter(\"source IN ('Gmail', 'Exchange', 'OutlookPlugin')\")\n",
					"    de_mod_dateDF = deDF.filter(\"source IN ('Box', 'GoogleDrive', 'SharePoint')\")\n",
					"\n",
					"    des_by_doc_v_lbl = join_on_vsid_doc_v_label(spark, de_doc_v_labelDF, sDF)\n",
					"    des_by_mod_date = join_on_vsid_mod_date(spark, de_mod_dateDF, sDF)\n",
					"\n",
					"    return des_by_doc_v_lbl.union(des_by_mod_date)\n",
					"\n",
					"def join_doc_spaces(spark, unrolled_events, spacesDF):\n",
					"    doc_eventsDF = convert_events(unrolled_events)\n",
					"    de_sidDF = doc_eventsDF.filter(\"space_id IS NOT NULL\")\n",
					"    de_vsidDF = doc_eventsDF.filter(\"version_series_id IS NOT NULL\")\n",
					"    spacesDF = (spacesDF\n",
					"                .select('id', 'organization_id', 'version_series_id',\n",
					"                        'modified_at', 'docurated_version_label'))\n",
					"    spacesDF.cache()\n",
					"\n",
					"    des_by_sidDF = join_on_sid(spark, de_sidDF, spacesDF)\n",
					"    des_by_vsidDF = join_on_vsid(spark, de_vsidDF, spacesDF)\n",
					"\n",
					"    return (des_by_vsidDF\n",
					"            .join(des_by_sidDF, 'event_skey', 'outer')\n",
					"            .select(\n",
					"              'event_skey',\n",
					"              coalesce(des_by_sidDF.space_id, des_by_vsidDF.space_id)\n",
					"              .alias('space_id'),\n",
					"            ))\n",
					"\n",
					"def event_docs_dim(ev_spaces, spaceDim, clusterDim):\n",
					"    norm_spaceDim = (spaceDim\n",
					"                     .select('skey', 'id', 'organization_id')\n",
					"                     .withColumn('space_id_copy', spaceDim.id)\n",
					"                     .withColumnRenamed('skey', 'space_skey')\n",
					"                     .withColumnRenamed('id', 'space_id'))\n",
					"    cr_spaceDim = (spaceDim\n",
					"                   .select(\n",
					"                       spaceDim.id.alias('cr_space_id'),\n",
					"                       spaceDim.name.alias('cr_space_name'),\n",
					"                       spaceDim.space_key.alias('cr_space_key'),\n",
					"                       spaceDim.modified_at.alias('cr_modified_at'),\n",
					"                       spaceDim.upload_source.alias('cr_upload_source'),\n",
					"                       spaceDim.source_file_type.alias('cr_content_type')))\n",
					"    clusterDim = (clusterDim\n",
					"                  .select(\n",
					"                      clusterDim.cluster_id.alias('cluster_id_copy'),\n",
					"                      'space_id',\n",
					"                      'cr_space_id'))\n",
					"    return (ev_spaces\n",
					"            .join(norm_spaceDim, 'space_id')\n",
					"            .join(clusterDim, 'space_id', 'left_outer')\n",
					"            .withColumn('cr_space_id',\n",
					"                        when(\n",
					"                            clusterDim.cluster_id_copy.isNull(),\n",
					"                            norm_spaceDim.space_id_copy\n",
					"                        ).otherwise(clusterDim.cr_space_id))\n",
					"            .join(cr_spaceDim, 'cr_space_id')\n",
					"            .withColumn('cluster_id',\n",
					"                        when(clusterDim.cluster_id_copy.isNull(),\n",
					"                             col('space_id'))\n",
					"                        .otherwise(clusterDim.cluster_id_copy))\n",
					"            .select('event_skey', 'space_skey', 'space_id', 'cluster_id',\n",
					"                    'cr_space_id', 'cr_space_key', 'cr_space_name',\n",
					"                    'cr_modified_at', 'cr_upload_source', 'cr_content_type',\n",
					"                    'organization_id')  # this is only for splitting purposes\n",
					"            .distinct()\n",
					"            .withColumn(\"skey\", monotonically_increasing_id()))\n",
					"\n",
					"def generate_event_documents(spark, unrolled_events, dims):\n",
					"    event_spaces = join_doc_spaces(spark, unrolled_events, dims['spaces'])\n",
					"    return event_docs_dim(event_spaces, dims['spaces'], dims['clusters'])\n",
					"\n",
					"def modify_opp_amount(opps):\n",
					"    return (opps\n",
					"            .withColumn('amount_new',\n",
					"                when(col('nlr__cloud__c').isNotNull(),\n",
					"                     col('nlr__cloud__c'))\n",
					"            .otherwise(col('amount')))\n",
					"            .drop('amount', 'nlr__cloud__c')\n",
					"            .withColumnRenamed('amount_new', 'amount'))\n",
					"\n",
					"def generate_share_opp_events(unrolled_events, sf_users):\n",
					"    users = sf_users.select('organization_id', 'email')\n",
					"    return (unrolled_events\n",
					"            .filter(\n",
					"                \"\"\"\n",
					"                source IN ('Exchange', 'Gmail', 'OutlookPlugin')\n",
					"                OR event_name = 'presentation_create_send-email'\n",
					"                \"\"\")\n",
					"            .join(users, ['organization_id', 'email']))\n",
					"\n",
					"def generate_user_joins_events(unrolled_events):\n",
					"    return (unrolled_events.filter(\n",
					"                            \"\"\"\n",
					"                            event_name = 'presentation_remote_updates_user-joins'\n",
					"                            \"\"\"))\n",
					"\n",
					"def generate_event_domains(opp_events):\n",
					"    return (opp_events\n",
					"            .withColumn('rec', explode('recipients'))\n",
					"            .select(\n",
					"                col('skey').alias('event_skey'),\n",
					"                col('rec.recipient_domain').alias('domain'),\n",
					"                col('rec.recipient_email').alias('email'),\n",
					"                'organization_id',\n",
					"                'event_name',\n",
					"                'time')\n",
					"            .distinct())\n",
					"\n",
					"def generate_opp_domains(spark, opps, contacts, sf_oppcontroles_dim):\n",
					"    bl = ' AND '.join([\"domain != '%s'\" % b for b in DOMAIN_BLACKLIST])\n",
					"    org_bl_df = org_domain_bl(spark)\n",
					"    org_multi_domain_bl_df = get_multi_domains_bl(contacts)\n",
					"    opp_contact_role = (sf_oppcontroles_dim\n",
					"                        .withColumn('epoch_time',\n",
					"                            unix_timestamp('created_date',TIME_FMT_MS))\n",
					"                        .select(['organization_id','contact_id', 'opportunity_id', 'epoch_time']))\n",
					"    contacts = (contacts\n",
					"                .join(opp_contact_role\n",
					"                .withColumnRenamed('contact_id', 'salesforce_id'),\n",
					"                ['salesforce_id', 'organization_id'] ))\n",
					"    c = (contacts\n",
					"         .withColumn('domain', lower(split('email', '@').getItem(1)))\n",
					"         .filter(\"domain IS NOT NULL AND domain != ''\")\n",
					"         .filter(bl)\n",
					"         .select('organization_id', 'account_id', 'email', 'domain', 'opportunity_id', 'epoch_time')\n",
					"         .join(org_bl_df, ['organization_id', 'domain'], 'left')\n",
					"         .filter(\"blacklisted IS NULL\")\n",
					"         .drop('blacklisted')\n",
					"         .join(org_multi_domain_bl_df, ['organization_id', 'domain'], 'left')\n",
					"         .filter(\"blacklisted IS NULL\")\n",
					"         .drop('blacklisted')\n",
					"         .distinct())\n",
					"\n",
					"    return (opps\n",
					"            .withColumnRenamed('salesforce_id', 'opportunity_id')\n",
					"            .join(c, ['organization_id', 'account_id', 'opportunity_id'])\n",
					"            .withColumnRenamed('opportunity_id', 'salesforce_id'))\n",
					"\n",
					"def generate_unstaged_event_opps(event_domains, opps_with_dates):\n",
					"    event_opps = (event_domains\n",
					"                    .join(opps_with_dates, ['organization_id', 'email'])\n",
					"                    .filter(\"time >= effective_created AND time <= effective_closed\")\n",
					"                    .withColumn('time_date', split(col('time'),' ')[0])\n",
					"                    .filter(\"time_date <= close_date\")\n",
					"                    .drop('effective_created', 'effective_closed')\n",
					"                    .withColumnRenamed('salesforce_id', 'opportunity_id')\n",
					"                    .select('event_skey', 'organization_id', 'opportunity_id', 'time', 'epoch_time', 'event_name')\n",
					"                    .distinct())\n",
					"    event_opps_valid = (event_opps\n",
					"                        .withColumn('epoch_event_time',\n",
					"                                unix_timestamp('time', TIME_FMT))\n",
					"                        .withColumn('difference', col('epoch_event_time') - col('epoch_time'))\n",
					"                        .filter('difference >= 0')\n",
					"                        .groupby(['event_skey', 'organization_id', 'opportunity_id'])\n",
					"                        .min('difference'))\n",
					"    return  (event_opps\n",
					"            .join(event_opps_valid, ['event_skey', 'organization_id', 'opportunity_id'])\n",
					"            .drop('min(difference)', 'epoch_event_time', 'epoch_time'))\n",
					"\n",
					"def generate_attach_event_opps(unrolled_events, sf_users):\n",
					"    users = sf_users.select('organization_id', 'email')\n",
					"    return (\n",
					"        unrolled_events\n",
					"        .filter(\n",
					"            \"\"\"\n",
					"            event_name = 'sfdc/canvas/attachment_attach_to_opportunity'\n",
					"            OR event_name = 'sfdc/canvas/attachment_attach_to_object'\n",
					"            OR event_name = 'sfdc/canvas/clipboard_attach_to_object'\n",
					"            \"\"\")\n",
					"        .filter(\"opportunity_id IS NOT NULL\")\n",
					"        .join(users, ['organization_id', 'email'])\n",
					"        .select(\n",
					"            col('skey').alias('event_skey'),\n",
					"            'organization_id',\n",
					"            'opportunity_id',\n",
					"            'event_name',\n",
					"            'time'))\n",
					"\n",
					"def add_attach_opps_time_filter(attach_event_opps, opps_with_dates):\n",
					"    cond = ((attach_event_opps.opportunity_id == opps_with_dates.salesforce_id) & (attach_event_opps.organization_id == opps_with_dates.organization_id))\n",
					"    return (attach_event_opps\n",
					"            .join(opps_with_dates, cond)\n",
					"            .filter(\"time <= effective_closed\")\n",
					"            .drop('effective_created', 'effective_closed')\n",
					"            .select('event_skey', opps_with_dates.organization_id, 'opportunity_id', 'time', 'event_name')\n",
					"            .distinct())\n",
					"\n",
					"def add_opp_stage(event_opps, opp_history):\n",
					"    ev_opps = event_opps.drop('stage_name')  # current stage_name of the opp\n",
					"    opp_hist = (opp_history\n",
					"                .withColumn('opp_hist_date',\n",
					"                            date_format('created_date', TIME_FMT))\n",
					"                .withColumn('created_date_secs',\n",
					"                            unix_timestamp('created_date', TIME_FMT_MS))\n",
					"                .select('opp_hist_date', 'organization_id', 'opportunity_id',\n",
					"                        'stage_name', 'created_date_secs'))\n",
					"    staged = (ev_opps\n",
					"              .join(opp_hist, ['organization_id', 'opportunity_id'])\n",
					"              .filter('time >= opp_hist_date')\n",
					"              .groupBy('event_skey', 'organization_id', 'opportunity_id')\n",
					"              .agg(max('created_date_secs').alias('created_date_secs'))\n",
					"              .join(opp_hist,\n",
					"                    ['organization_id', 'opportunity_id', 'created_date_secs'])\n",
					"              .select('event_skey', 'opportunity_id', 'opp_hist_date',\n",
					"                      'stage_name')\n",
					"              .groupBy('event_skey', 'opportunity_id', 'opp_hist_date')\n",
					"              .agg(first('stage_name').alias('stage_name')))\n",
					"    return (ev_opps\n",
					"            .join(staged, ['event_skey', 'opportunity_id'], 'left')\n",
					"            .select(\n",
					"                'event_skey', 'organization_id', 'opportunity_id',\n",
					"                'stage_name', 'event_name'))\n",
					"\n",
					"def add_attribution(event_opps, opps):\n",
					"    filtered_event_opps = event_opps.filter(\"event_name = 'presentation_remote_updates_user-joins'\")\n",
					"    event_opp_counts = (filtered_event_opps\n",
					"                        .groupBy('organization_id', 'opportunity_id')\n",
					"                        .count()\n",
					"                        .withColumnRenamed('count', 'event_count'))\n",
					"    opp_amounts = (opps\n",
					"                   .select(\n",
					"                       'organization_id',\n",
					"                       col('salesforce_id').alias('opportunity_id'),\n",
					"                       when(expr('is_closed = TRUE AND is_won = TRUE'),\n",
					"                            col('amount'))\n",
					"                       .otherwise(0.0).cast('double').alias('amount')))\n",
					"    events_with_count = (event_opps\n",
					"                         .join(event_opp_counts, ['organization_id', 'opportunity_id']))\n",
					"    events_with_count = (events_with_count.withColumn('updated_event_count',\n",
					"                                                      when(expr(\"event_name = 'presentation_remote_updates_user-joins'\"), col('event_count'))\n",
					"                                                      .otherwise(0)).drop('event_count').withColumnRenamed('updated_event_count', 'event_count'))\n",
					"\n",
					"    return (events_with_count\n",
					"            .join(opp_amounts, ['organization_id', 'opportunity_id'])\n",
					"            .withColumn(\n",
					"                'attribution',\n",
					"                when(expr('event_count IS NOT NULL AND event_count > 0'),\n",
					"                     col('amount')/col('event_count'))\n",
					"                .otherwise(0.0).cast('double'))\n",
					"            .drop('event_count', 'amount'))\n",
					"\n",
					"\n",
					"\n",
					"def generate_event_opps(spark, unrolled_events, dims):\n",
					"    opps = dims['sf_opportunities']\n",
					"    contacts = dims['sf_contacts']\n",
					"    opp_history = dims['sf_opp_histories']\n",
					"    sf_users = dims['sf_users']\n",
					"    sf_oppcontroles_dim = dims['sf_oppcontroles']\n",
					"\n",
					"    opps = modify_opp_amount(opps)\n",
					"    share_opp_events = generate_share_opp_events(unrolled_events, sf_users)\n",
					"    event_domains_share = generate_event_domains(share_opp_events)\n",
					"\n",
					"    user_joins_events = generate_user_joins_events(unrolled_events)\n",
					"    event_domains_uj = generate_event_domains(user_joins_events)\n",
					"    event_domains = (event_domains_share.union(event_domains_uj).distinct())\n",
					"\n",
					"    opp_with_contacts = generate_opp_domains(spark, opps, contacts, sf_oppcontroles_dim)\n",
					"    opps_with_dates = add_opp_dates(opp_with_contacts, opp_history)\n",
					"\n",
					"    event_opps = generate_unstaged_event_opps(event_domains, opps_with_dates)\n",
					"\n",
					"    attach_event_opps = generate_attach_event_opps(unrolled_events, sf_users)\n",
					"    attach_opps_event_time_filter = add_attach_opps_time_filter(attach_event_opps, opps_with_dates)\n",
					"    event_opps = event_opps.union(attach_opps_event_time_filter)\n",
					"\n",
					"    event_opps_staged = add_opp_stage(event_opps, opp_history)\n",
					"    event_opps_staged_attr = add_attribution(event_opps_staged, opps)\n",
					"    return event_opps_staged_attr\n",
					"\n",
					"\n",
					"\n",
					"def generate_user_event_opps(user_events, ev_docs, ev_opps, dims):\n",
					"    evDocs = (ev_docs\n",
					"              .select(\n",
					"                  'event_skey',\n",
					"                  'space_skey',\n",
					"                  'cluster_id',\n",
					"                  'cr_space_id',\n",
					"                  'cr_space_key',\n",
					"                  'cr_space_name',\n",
					"                  'cr_modified_at',\n",
					"                  'cr_upload_source'\n",
					"              ))\n",
					"    oppDim = (dims['sf_opportunities']\n",
					"              .select(\n",
					"                  'organization_id',\n",
					"                  col('skey').alias('opportunity_skey'),\n",
					"                  col('salesforce_id').alias('opportunity_id'),\n",
					"                  'account_id',\n",
					"                  'is_closed',\n",
					"                  'is_won'))\n",
					"    acctDim = (dims['sf_accounts']\n",
					"               .select(\n",
					"                   'organization_id',\n",
					"                   col('salesforce_id').alias('account_id'),\n",
					"                   'industry'))\n",
					"    return (ev_opps\n",
					"            .join(evDocs, 'event_skey')\n",
					"            .join(oppDim, ['organization_id', 'opportunity_id'])\n",
					"            .join(acctDim, ['organization_id', 'account_id'])\n",
					"            .withColumn('effectiveness',\n",
					"                        when(expr(\"is_closed = TRUE AND is_won = TRUE\"), 1)\n",
					"                        .otherwise(0))\n",
					"            .select(\n",
					"                'event_skey',\n",
					"                'space_skey',\n",
					"                'cluster_id',\n",
					"                'cr_space_id',\n",
					"                'cr_space_key',\n",
					"                'cr_space_name',\n",
					"                'cr_modified_at',\n",
					"                'cr_upload_source',\n",
					"                'opportunity_skey',\n",
					"                'opportunity_id',\n",
					"                'industry',\n",
					"                'effectiveness',\n",
					"                'attribution',\n",
					"                'stage_name')\n",
					"            .join(user_events, 'event_skey'))\n",
					"\n",
					"def convert_df_type_to_sql(df_type):\n",
					"    if df_type == 'integer':\n",
					"        return FIELD_INT\n",
					"    elif df_type == 'float':\n",
					"        return FIELD_FLOAT\n",
					"    elif df_type == 'double':\n",
					"        return FIELD_DECIMAL\n",
					"    elif df_type == 'long':\n",
					"        return FIELD_LONG\n",
					"    elif df_type == 'timestamp':\n",
					"        return FIELD_TIMESTAMP\n",
					"    elif df_type == 'boolean':\n",
					"        return FIELD_BOOLEAN\n",
					"    elif df_type == 'string':\n",
					"        return FIELD_STRING\n",
					"    else:\n",
					"        return FIELD_STRING\n",
					"\n",
					"def create_schema_from_df(df):\n",
					"    cols = []\n",
					"    for idx, f in enumerate(df.schema):\n",
					"        f_json = f.jsonValue()\n",
					"        sql_type = convert_df_type_to_sql(f_json['type'])\n",
					"        c = [f_json['name'], sql_type, idx, f_json['nullable']]\n",
					"        cols.append(c)\n",
					"    return cols\n",
					"\n",
					"\n",
					"CSV_WRITE_OPTS = { \n",
					"    'mode': 'overwrite', \n",
					"    'escape': '\"', \n",
					"    'quoteAll': True,\n",
					"    'timestampFormat' : 'yyyy-MM-dd HH:mm:ss' }\n",
					"\n",
					"def write_df(spark, df, name, folder, opts={}, schema=False):\n",
					"    ext = 'csv'\n",
					"    write_opts = {}\n",
					"    write_opts = CSV_WRITE_OPTS\n",
					"    write_opts = write_opts.copy()\n",
					"\n",
					"    fn = os.path.join(folder, '%s_dump' % name)\n",
					"    if ext == 'csv':\n",
					"        df.write.csv(fn, **write_opts)\n",
					"    elif ext == 'json':\n",
					"        df.write.json(fn, **write_opts)\n",
					"    if schema:\n",
					"        write_df_schema(spark, df, name, folder)\n",
					"\n",
					"def write_df_schema(spark, df, name, folder):\n",
					"    json_schema = json.dumps(create_schema_from_df(df))\n",
					"    rdd = spark.sparkContext.parallelize([[json_schema]])\n",
					"    temp_schema = StructType([StructField('schema', StringType(), False)])\n",
					"    schema_df = spark.createDataFrame(rdd, temp_schema)\n",
					"    fn = os.path.join(folder, '%s_schema' % name)\n",
					"    schema_df.coalesce(1).write.mode('overwrite').text(fn)\n",
					"    return\n",
					"\n",
					"def write_df_and_schema(spark, df, name, work_dir, output_dir):\n",
					"    write_df(spark, df, name, output_dir)\n",
					"    write_df_schema(spark, df, name, work_dir)\n",
					"\n",
					"user_events = generate_user_events(unrolled_events, dims)\n",
					"event_docs = generate_event_documents(spark, unrolled_events, dims)\n",
					"event_opps = generate_event_opps(spark, unrolled_events, dims)\n",
					"\n",
					"user_event_opps = generate_user_event_opps(user_events, event_docs,\n",
					"                                               event_opps, dims)\n",
					"write_df_and_schema(spark, user_event_opps, 'user_event_opportunities',\n",
					"                        '/kartik', '/kartik')"
				],
				"execution_count": 15
			}
		]
	}
}